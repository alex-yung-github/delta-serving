{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Tiny layer params: 33472\n",
      "Changed elements: 994 / 33472 (~2.97%)\n",
      "=== Compression Stats ===\n",
      "Total param bytes  : 133888\n",
      "Total delta bytes  : 133888\n",
      "Total compressed   : 5633\n",
      "Global ratio       : 0.04207247848948375\n",
      "- ln1.weight                               | param=256 | comp=27 | ratio=0.1055\n",
      "- ln1.bias                                 | param=256 | comp=26 | ratio=0.1016\n",
      "- self_attn.in_proj_weight                 | param=49152 | comp=1963 | ratio=0.0399\n",
      "- self_attn.in_proj_bias                   | param=768 | comp=53 | ratio=0.0690\n",
      "- self_attn.out_proj.weight                | param=16384 | comp=707 | ratio=0.0432\n",
      "=== Component-wise Timing (seconds): mean ± std over 7 runs ===\n",
      "    t_load_pkg: 0.000015 ± 0.000002\n",
      "  t_decompress: 0.000044 ± 0.000007\n",
      "   t_xor_apply: 0.000057 ± 0.000015\n",
      "  t_state_load: 0.000404 ± 0.000081\n",
      "     t_forward: 0.000446 ± 0.000525\n",
      "  t_end_to_end: 0.000967 ± 0.000560\n",
      "Sanity checksums (first 3): [3409.4951171875, 3409.4951171875, 3409.4951171875]\n",
      "=== What-if Speed Windows (vs current E2E mean) ===\n",
      "Baseline E2E: 0.000967s\n",
      "Skip XOR (keep decompress): 0.000909s  → 1.06× faster (↓5.9%)\n",
      "Skip XOR+Decompress (Zstd-domain compute): 0.000865s  → 1.12× faster (↓10.5%)\n",
      "\n",
      "Interpretation:\n",
      "- (Baseline → Bound A) is the best-case improvement if we avoid the global XOR-apply step by\n",
      "  computing directly on decompressed XOR bytes (tile JIT patching, patch-based kernels, etc.).\n",
      "- (Baseline → Bound B) is a theoretical ceiling if we could avoid both decompression and XOR\n",
      "  (i.e., compute in Zstd domain); not realistic today, but shows maximum headroom.\n",
      "=== Top 5 params by XOR-apply time ===\n",
      "self_attn.in_proj_weight                 | bytes=   49152 | decompress=0.000012s | xor=0.000016s\n",
      "ff.0.weight                              | bytes=   32768 | decompress=0.000008s | xor=0.000010s\n",
      "ff.2.weight                              | bytes=   32768 | decompress=0.000007s | xor=0.000010s\n",
      "ln1.weight                               | bytes=     256 | decompress=0.000002s | xor=0.000010s\n",
      "self_attn.out_proj.weight                | bytes=   16384 | decompress=0.000003s | xor=0.000007s\n",
      "\n",
      "=== XOR Sparsity Estimate ===\n",
      "xor_total_bytes             : 133888\n",
      "xor_nonzero_bytes           : 3138\n",
      "nonzero_density             : 0.0234375\n",
      "dense_storage_bytes         : 133888\n",
      "simple_sparse_storage_bytes : 15690\n",
      "sparse_over_dense_ratio     : 0.1171875\n",
      "\n",
      "NOTE: If non-zero density is low, consider block-sparse indexing (e.g., 256B/4KB blocks), RLE on zero-runs, or bitplane packing so **many XOR adapters can co-reside on a single GPU.**\n"
     ]
    }
   ],
   "source": [
    "# Given naive approach benchmarks\n",
    "# If needed, install deps:\n",
    "# !pip install torch zstandard\n",
    "\n",
    "import math, io, pickle, time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    import zstandard as zstd\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"Please install `zstandard` (pip install zstandard) before running this cell.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny Transformer layer\n",
    "# -----------------------------\n",
    "class TinyTransformerLayer(nn.Module):\n",
    "    \"\"\"Small block with MHA + FFN, sufficient for demonstration.\"\"\"\n",
    "    def __init__(self, d_model=64, nhead=4, dim_feedforward=128, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.self_attn(h, h, h, attn_mask=attn_mask)\n",
    "        x = x + a\n",
    "        h2 = self.ln2(x)\n",
    "        y = self.ff(h2)\n",
    "        return x + y\n",
    "\n",
    "base_layer = TinyTransformerLayer().eval()\n",
    "print(\"Tiny layer params:\", sum(p.numel() for p in base_layer.parameters()))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simulate finetuning (sparse updates)\n",
    "# -----------------------------\n",
    "from copy import deepcopy\n",
    "\n",
    "def perturb_subset_(module: nn.Module, frac: float = 0.03, magnitude: float = 1e-2, seed: int = 0):\n",
    "    \"\"\"In-place: perturb `frac` of elements in each float parameter.\n",
    "    This simulates sparse finetuning, making XOR(base, finetuned) mostly zeros.\n",
    "    \"\"\"\n",
    "    g = torch.Generator(device='cpu').manual_seed(seed)\n",
    "    with torch.no_grad():\n",
    "        for name, p in module.named_parameters():\n",
    "            if not p.dtype.is_floating_point:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            k = max(1, int(numel * frac))\n",
    "            idx = torch.randperm(numel, generator=g, device=p.device)[:k]\n",
    "            flat = p.view(-1)\n",
    "            noise = magnitude * torch.randn(k, generator=g, device=p.device, dtype=flat.dtype)\n",
    "            flat[idx] += noise\n",
    "\n",
    "finetuned_layer = deepcopy(base_layer)\n",
    "perturb_subset_(finetuned_layer, frac=0.03, magnitude=1e-2, seed=123)\n",
    "\n",
    "# Count changed elements (exact because we add noise)\n",
    "changed, total = 0, 0\n",
    "for (n1, p_base), (n2, p_ft) in zip(base_layer.named_parameters(), finetuned_layer.named_parameters()):\n",
    "    assert n1 == n2\n",
    "    if p_base.dtype.is_floating_point:\n",
    "        diff = (p_base != p_ft).sum().item()\n",
    "        changed += diff\n",
    "        total += p_base.numel()\n",
    "print(f\"Changed elements: {changed} / {total} (~{changed/total*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR + Zstd compression helpers\n",
    "# -----------------------------\n",
    "def tensor_to_uint8_view(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Return a NumPy uint8 *view* over tensor bytes (CPU).\"\"\"\n",
    "    a = t.detach().cpu().numpy()\n",
    "    return a.view(np.uint8)\n",
    "\n",
    "def xor_uint8(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    assert a.dtype == np.uint8 and b.dtype == np.uint8 and a.size == b.size\n",
    "    return np.bitwise_xor(a, b)\n",
    "\n",
    "def compress_state_dict_with_xor_zstd(base_sd: dict, finetuned_sd: dict, level: int = 10):\n",
    "    \"\"\"Pack XOR(Zstd) deltas.\n",
    "    Returns a dict with:\n",
    "      - meta[name]: shape/dtype/numel/param_bytes/uint8_shape\n",
    "      - deltas[name]: Zstd-compressed XOR bytes\n",
    "      - stats: aggregate sizes\n",
    "    \"\"\"\n",
    "    compressor = zstd.ZstdCompressor(level=level)\n",
    "    meta, deltas = {}, {}\n",
    "    stats = {'total_params_bytes': 0, 'total_delta_bytes': 0, 'total_compressed_bytes': 0, 'per_param': {}}\n",
    "\n",
    "    for name, base_t in base_sd.items():\n",
    "        ft_t = finetuned_sd[name]\n",
    "        base_bytes = tensor_to_uint8_view(base_t)\n",
    "        ft_bytes = tensor_to_uint8_view(ft_t)\n",
    "        assert base_bytes.size == ft_bytes.size, name\n",
    "\n",
    "        delta_u8 = xor_uint8(base_bytes, ft_bytes)\n",
    "        raw_delta = delta_u8.tobytes()\n",
    "        comp = compressor.compress(raw_delta)\n",
    "\n",
    "        meta[name] = {\n",
    "            'shape': list(ft_t.shape),\n",
    "            'dtype_str': str(ft_t.detach().cpu().numpy().dtype.str),  # e.g. '<f4'\n",
    "            'numel': ft_t.numel(),\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'uint8_shape': list(base_bytes.shape),\n",
    "        }\n",
    "        deltas[name] = comp\n",
    "\n",
    "        stats['total_params_bytes'] += ft_bytes.size\n",
    "        stats['total_delta_bytes'] += len(raw_delta)\n",
    "        stats['total_compressed_bytes'] += len(comp)\n",
    "        stats['per_param'][name] = {\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'delta_bytes': len(raw_delta),\n",
    "            'compressed_bytes': len(comp),\n",
    "            'compressed_ratio_vs_param': len(comp) / max(1, ft_bytes.size),\n",
    "        }\n",
    "\n",
    "    return {'meta': meta, 'deltas': deltas, 'zstd_level': level, 'stats': stats}\n",
    "\n",
    "def reconstruct_finetuned_state_dict_with_timing(base_sd: dict, pkg: dict):\n",
    "    \"\"\"Decompress and XOR-apply with timing breakdown.\n",
    "    Returns: (rec_sd, timing_dict) with keys: t_decompress, t_xor_apply\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    rec = {}\n",
    "    t_decomp = 0.0\n",
    "    t_xor = 0.0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        # Decompress\n",
    "        t0 = perf_counter()\n",
    "        raw_delta = dctx.decompress(comp)\n",
    "        t_decomp += perf_counter() - t0\n",
    "        delta_u8 = np.frombuffer(raw_delta, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "\n",
    "        # XOR apply\n",
    "        base_bytes = tensor_to_uint8_view(base_sd[name])\n",
    "        t1 = perf_counter()\n",
    "        ft_bytes = np.bitwise_xor(base_bytes, delta_u8)  # shape-aligned XOR\n",
    "        t_xor += perf_counter() - t1\n",
    "\n",
    "        # Reinterpret as original dtype/shape\n",
    "        arr = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(meta['shape']).copy()\n",
    "        rec[name] = torch.from_numpy(arr)\n",
    "    return rec, {'t_decompress': t_decomp, 't_xor_apply': t_xor}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build base/finetuned SDs and compress\n",
    "# -----------------------------\n",
    "base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "ft_sd   = {k: v.detach().cpu().contiguous() for k, v in finetuned_layer.state_dict().items()}\n",
    "\n",
    "pkg = compress_state_dict_with_xor_zstd(base_sd, ft_sd, level=10)\n",
    "print(\"=== Compression Stats ===\")\n",
    "print(\"Total param bytes  :\", pkg['stats']['total_params_bytes'])\n",
    "print(\"Total delta bytes  :\", pkg['stats']['total_delta_bytes'])\n",
    "print(\"Total compressed   :\", pkg['stats']['total_compressed_bytes'])\n",
    "print(\"Global ratio       :\", pkg['stats']['total_compressed_bytes']/pkg['stats']['total_params_bytes'])\n",
    "for name, st in list(pkg['stats']['per_param'].items())[:5]:\n",
    "    print(f\"- {name:40s} | param={st['param_bytes']} | comp={st['compressed_bytes']} | ratio={st['compressed_ratio_vs_param']:.4f}\")\n",
    "\n",
    "# Serialize to in-memory bytes (so we can measure \"load\" as deserialization;\n",
    "# replace with actual disk I/O in your system if you want true file load timings).\n",
    "pkg_bytes = pickle.dumps(pkg)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end function with timers (load → reconstruct → load_state_dict → forward)\n",
    "# -----------------------------\n",
    "def e2e_forward_from_pkg_bytes(base_layer: nn.Module, pkg_bytes: bytes, x: torch.Tensor, device: str = 'cpu'):\n",
    "    \"\"\"End-to-end: load (deserialize) → decompress → XOR-apply → load_state_dict → forward.\n",
    "    Returns (timings_dict, checksum).\n",
    "    NOTE: 't_load_pkg' here measures pickle.loads (in-memory). Replace with disk I/O for on-disk timings.\n",
    "    \"\"\"\n",
    "    # 0) Prepare inputs\n",
    "    x = x.to(device)\n",
    "    base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "\n",
    "    # 1) Load package (deserialize)\n",
    "    t0 = perf_counter()\n",
    "    pkg = pickle.loads(pkg_bytes)\n",
    "    t_load = perf_counter() - t0\n",
    "\n",
    "    # 2) Decompress + XOR apply (split timings)\n",
    "    rec_sd, t_parts = reconstruct_finetuned_state_dict_with_timing(base_sd, pkg)\n",
    "\n",
    "    # 3) Materialize a new layer and copy weights\n",
    "    t2 = perf_counter()\n",
    "    rec_layer = TinyTransformerLayer().to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for k, p in rec_layer.state_dict().items():\n",
    "            p.copy_(rec_sd[k].to(device))\n",
    "    t_state = perf_counter() - t2\n",
    "\n",
    "    # 4) Forward once (demo)\n",
    "    t3 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        y = rec_layer(x)\n",
    "    t_fwd = perf_counter() - t3\n",
    "\n",
    "    timings = {\n",
    "        't_load_pkg': t_load,\n",
    "        't_decompress': t_parts['t_decompress'],\n",
    "        't_xor_apply': t_parts['t_xor_apply'],\n",
    "        't_state_load': t_state,\n",
    "        't_forward': t_fwd,\n",
    "        't_end_to_end': t_load + t_parts['t_decompress'] + t_parts['t_xor_apply'] + t_state + t_fwd,\n",
    "    }\n",
    "    checksum = float(y.abs().sum().detach().cpu())  # simple checksum to ensure consistent output\n",
    "    return timings, checksum\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Micro-benchmark: mean ± std per stage\n",
    "# -----------------------------\n",
    "def benchmark_e2e_breakdown(base_layer, pkg_bytes: bytes, x: torch.Tensor, device: str = 'cpu', repeats: int = 7):\n",
    "    \"\"\"Repeat the E2E pipeline `repeats` times and report mean/std for each stage.\"\"\"\n",
    "    keys = ['t_load_pkg','t_decompress','t_xor_apply','t_state_load','t_forward','t_end_to_end']\n",
    "    buf = {k: [] for k in keys}\n",
    "    checks = []\n",
    "    for _ in range(repeats):\n",
    "        timings, checksum = e2e_forward_from_pkg_bytes(base_layer, pkg_bytes, x, device=device)\n",
    "        for k in keys:\n",
    "            buf[k].append(timings[k])\n",
    "        checks.append(checksum)\n",
    "    stats = {k: {'mean': float(np.mean(v)), 'std': float(np.std(v))} for k, v in buf.items()}\n",
    "    return stats, checks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# What-if speed windows\n",
    "# -----------------------------\n",
    "def print_what_if_bounds(stats):\n",
    "    \"\"\"Compute what-if bounds from measured means:\n",
    "       - Bound A: skip global XOR (still decompress) → compute on decompressed XOR bytes\n",
    "       - Bound B: skip XOR + decompress (theoretical Zstd-domain compute)\n",
    "    \"\"\"\n",
    "    t_load = stats['t_load_pkg']['mean']\n",
    "    t_decomp = stats['t_decompress']['mean']\n",
    "    t_xor = stats['t_xor_apply']['mean']\n",
    "    t_state = stats['t_state_load']['mean']\n",
    "    t_fwd = stats['t_forward']['mean']\n",
    "    t_e2e = stats['t_end_to_end']['mean']\n",
    "\n",
    "    t_boundA = t_load + t_decomp + t_state + t_fwd       # skip XOR\n",
    "    t_boundB = t_load + t_state + t_fwd                  # skip XOR + decompress (theoretical)\n",
    "\n",
    "    def fmt_speedup(old, new):\n",
    "        return f\"{old/new:.2f}× faster (↓{(1 - new/old)*100:.1f}%)\" if new > 0 else \"∞\"\n",
    "\n",
    "    print(\"=== What-if Speed Windows (vs current E2E mean) ===\")\n",
    "    print(f\"Baseline E2E: {t_e2e:.6f}s\")\n",
    "    print(f\"Skip XOR (keep decompress): {t_boundA:.6f}s  → {fmt_speedup(t_e2e, t_boundA)}\")\n",
    "    print(f\"Skip XOR+Decompress (Zstd-domain compute): {t_boundB:.6f}s  → {fmt_speedup(t_e2e, t_boundB)}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- (Baseline → Bound A) is the best-case improvement if we avoid the global XOR-apply step by\")\n",
    "    print(\"  computing directly on decompressed XOR bytes (tile JIT patching, patch-based kernels, etc.).\")\n",
    "    print(\"- (Baseline → Bound B) is a theoretical ceiling if we could avoid both decompression and XOR\")\n",
    "    print(\"  (i.e., compute in Zstd domain); not realistic today, but shows maximum headroom.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Per-parameter hotspots (identify where XOR time concentrates)\n",
    "# -----------------------------\n",
    "def per_param_decompress_xor_timing(base_sd: dict, pkg: dict, topk: int = 5):\n",
    "    \"\"\"Per-parameter timing for decompress and XOR-apply to identify hotspots.\"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    records = []\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        t0 = perf_counter(); raw = dctx.decompress(comp); t_de = perf_counter()-t0\n",
    "        delta = np.frombuffer(raw, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "        t1 = perf_counter(); _ = np.bitwise_xor(tensor_to_uint8_view(base_sd[name]), delta); t_x = perf_counter()-t1\n",
    "        records.append((name, meta['param_bytes'], t_de, t_x))\n",
    "    # sort by XOR time (or total)\n",
    "    records.sort(key=lambda r: r[3], reverse=True)\n",
    "    print(f\"=== Top {topk} params by XOR-apply time ===\")\n",
    "    for name, sz, tde, tx in records[:topk]:\n",
    "        print(f\"{name:40s} | bytes={sz:8d} | decompress={tde:.6f}s | xor={tx:.6f}s\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR sparsity & naive sparse storage estimate\n",
    "# -----------------------------\n",
    "def estimate_xor_sparsity_and_storage(pkg: dict):\n",
    "    \"\"\"Measure XOR sparsity (non-zero bytes) and estimate a simple sparse storage cost.\n",
    "    Model: store (index:uint32 + value:uint8) per non-zero byte.\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    total_bytes = 0\n",
    "    total_nz = 0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        raw = dctx.decompress(pkg['deltas'][name])\n",
    "        arr = np.frombuffer(raw, dtype=np.uint8)\n",
    "        nz = int((arr != 0).sum())\n",
    "        total_bytes += arr.size\n",
    "        total_nz += nz\n",
    "    density = total_nz / max(1, total_bytes)\n",
    "    dense_bytes = total_bytes  # 1 byte per entry\n",
    "    # Sparse: 4 bytes for index + 1 byte value (very simple model)\n",
    "    sparse_bytes = total_nz * (4 + 1)\n",
    "    return {\n",
    "        'xor_total_bytes': total_bytes,\n",
    "        'xor_nonzero_bytes': total_nz,\n",
    "        'nonzero_density': density,\n",
    "        'dense_storage_bytes': dense_bytes,\n",
    "        'simple_sparse_storage_bytes': sparse_bytes,\n",
    "        'sparse_over_dense_ratio': sparse_bytes / max(1, dense_bytes),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run benchmarks\n",
    "# -----------------------------\n",
    "# End-to-end breakdown\n",
    "x_bench = torch.randn(4, 16, 64)\n",
    "stats, checks = benchmark_e2e_breakdown(base_layer, pkg_bytes, x_bench, device=device, repeats=7)\n",
    "print(\"=== Component-wise Timing (seconds): mean ± std over 7 runs ===\")\n",
    "for k, s in stats.items():\n",
    "    print(f\"{k:>14s}: {s['mean']:.6f} ± {s['std']:.6f}\")\n",
    "print(\"Sanity checksums (first 3):\", checks[:3])\n",
    "\n",
    "# What-if bounds\n",
    "print_what_if_bounds(stats)\n",
    "\n",
    "# Hotspots\n",
    "_ = per_param_decompress_xor_timing(base_sd, pkg, topk=5)\n",
    "\n",
    "# XOR sparsity summary\n",
    "sparsity = estimate_xor_sparsity_and_storage(pkg)\n",
    "print(\"\\n=== XOR Sparsity Estimate ===\")\n",
    "for k, v in sparsity.items():\n",
    "    print(f\"{k:28s}: {v}\")\n",
    "print(\"\\nNOTE: If non-zero density is low, consider block-sparse indexing (e.g., 256B/4KB blocks), \"\n",
    "      \"RLE on zero-runs, or bitplane packing so **many XOR adapters can co-reside on a single GPU.**\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMM on zstd Compressed XORed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION: Tiled Hooks # -----------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def update_mha_bias_in_place(module, param_name, new_data, device):\n",
    "    \"\"\"Update the MHA bias parameter in-place with finetuned data.\"\"\"\n",
    "    print(f\"  Updated MHA bias parameter in-place for {param_name}\")\n",
    "    if 'in_proj_bias' in param_name:\n",
    "        module.in_proj_bias.data = new_data.to(module.in_proj_bias.device, module.in_proj_bias.dtype)\n",
    "    else:\n",
    "        module.out_proj.bias.data = new_data.to(module.out_proj.bias.device, module.out_proj.bias.dtype)\n",
    "    return None\n",
    "\n",
    "\n",
    "def track_timing(timing_data, tile_data, param_name, t_decompress, t_xor=0.0, t_forward=0.0, num_tiles=1):\n",
    "    \"\"\"Track the timing metrics for this parameter.\"\"\"\n",
    "    timing_data['param_names'].append(param_name)\n",
    "    timing_data['t_partial_decompress'].append(t_decompress)\n",
    "    timing_data['t_tile_xor_patch'].append(t_xor)\n",
    "    timing_data['t_forward_tile'].append(t_forward)\n",
    "    tile_data['param_names'].append(param_name)\n",
    "    tile_data['num_tiles'].append(num_tiles)\n",
    "\n",
    "\n",
    "def create_bias_norm_hook(param_name, module, pkg, device, timing_data, size_data, tile_data, is_bias, is_norm, is_mha):\n",
    "    \"\"\"Create hook for biases and norms.\"\"\"\n",
    "    print(f\"-----------------{param_name} Hook (Bias/Norm)-----------------\")\n",
    "    print(f\"  Flags: is_mha={is_mha}, is_bias={is_bias}, is_norm={is_norm}\")\n",
    "    if param_name not in pkg['deltas']:\n",
    "        print(f\"  Skipping {param_name}: no update in pkg['deltas']\")\n",
    "        return None\n",
    "\n",
    "    meta = pkg['meta'][param_name]\n",
    "    shape = meta['shape']\n",
    "    uint8_shape = meta['uint8_shape']\n",
    "    compressed_xor = pkg['deltas'][param_name]\n",
    "    print(f\"  Shape: {shape}, uint8_shape: {uint8_shape}, numel: {meta['numel']}, bytes: {meta['param_bytes']}\")\n",
    "\n",
    "    size_data['param_names'].append(param_name)\n",
    "    size_data['numel'].append(meta['numel'])\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    xor_bytes = zstd.decompress(compressed_xor)\n",
    "    xor_arr = np.frombuffer(xor_bytes, dtype=np.uint8).reshape(uint8_shape)\n",
    "    t_decompress = perf_counter() - t0\n",
    "    print(f\"  Decompressed XOR bytes in {t_decompress:.7f}s, shape: {xor_arr.shape}\")\n",
    "\n",
    "    if is_mha and is_bias:\n",
    "        base_data = (module.in_proj_bias if 'in_proj_bias' in param_name else module.out_proj_bias).data.cpu().numpy()\n",
    "    else:\n",
    "        base_data = (module.bias if is_bias else module.weight).data.cpu().numpy()\n",
    "    print(f\"  Base data shape: {base_data.shape}, dtype: {base_data.dtype}\")\n",
    "\n",
    "    base_bytes = base_data.view(np.uint8).reshape(uint8_shape)\n",
    "    ft_bytes = np.bitwise_xor(base_bytes, xor_arr)\n",
    "    new_data = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(shape)\n",
    "    new_data = torch.from_numpy(new_data).to(dtype=torch.float32, device=device)\n",
    "    print(f\"  Finetuned data shape: {new_data.shape}, sample values: {new_data.flatten()[:5]}\")\n",
    "\n",
    "    track_timing(timing_data, tile_data, param_name, t_decompress)\n",
    "\n",
    "    if is_mha and is_bias:\n",
    "        print(f\"  DEBUG: is_mha={is_mha}, is_bias={is_bias} for {param_name} - updating in-place, NO HOOK\")\n",
    "        update_mha_bias_in_place(module, param_name, new_data, device)\n",
    "        return None\n",
    "\n",
    "    print(f\"  DEBUG: Creating forward hook for non-MHA {param_name}\")\n",
    "    def hook_fn(module, inputs, output):\n",
    "        print(f\"  Applying hook for {param_name} (shaped: {new_data.shape}), input shape: {inputs[0].shape}\")\n",
    "        if is_norm:\n",
    "            output = output * new_data\n",
    "            print(f\"  Applied norm multiplication (output *= finetuned weight)\")\n",
    "        else:\n",
    "            output_shape = output.shape if not is_mha else output[0].shape\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                batch, seq, dim = output_shape\n",
    "                if new_data.shape[-1] != dim:\n",
    "                    raise ValueError(f\"Bias shape {new_data.shape} does not match output dim {dim}\")\n",
    "                bias_expanded = new_data.unsqueeze(0).unsqueeze(1).expand(batch, seq, dim)\n",
    "                print(f\"  Applied LayerNorm bias addition (expanded to {bias_expanded.shape})\")\n",
    "            else:\n",
    "                if len(output_shape) == 3:\n",
    "                    batch, seq, dim = output_shape\n",
    "                    if new_data.shape[-1] != dim:\n",
    "                        raise ValueError(f\"Bias shape {new_data.shape} does not match output dim {dim}\")\n",
    "                    bias_expanded = new_data.unsqueeze(0).unsqueeze(1).expand(batch, seq, dim)\n",
    "                else:\n",
    "                    if new_data.shape[-1] != output_shape[-1]:\n",
    "                        raise ValueError(f\"Bias shape {new_data.shape} does not match output dim {output_shape[-1]}\")\n",
    "                    bias_expanded = new_data.unsqueeze(0).expand(output_shape[0], -1)\n",
    "                output = output + bias_expanded if not is_mha else (output[0] + bias_expanded, output[1])\n",
    "                print(f\"  Applied bias addition (expanded to {bias_expanded.shape})\")\n",
    "        print(f\"  Output shape after hook: {output.shape if not is_mha else output[0].shape}\")\n",
    "        return output\n",
    "\n",
    "    print(f\"  Created and returning hook function for {param_name}\")\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def create_weight_hook(param_name, module, pkg, device, timing_data, size_data, tile_data, is_mha, tile_cols=64, tile_rows=64):\n",
    "    \"\"\"Create hook for weights using full matrix multiplication (Sub-Step 4.2a).\"\"\"\n",
    "    print(f\"-----------------{param_name} Hook (Weight)-----------------\")\n",
    "    print(f\"  Flags: is_mha={is_mha}, is_bias=False, is_norm=False\")\n",
    "    if param_name not in pkg['deltas']:\n",
    "        print(f\"  Skipping {param_name}: no update in pkg['deltas']\")\n",
    "        return None\n",
    "\n",
    "    meta = pkg['meta'][param_name]\n",
    "    shape = meta['shape']\n",
    "    uint8_shape = meta['uint8_shape']\n",
    "    compressed_xor = pkg['deltas'][param_name]\n",
    "    print(f\"  Shape: {shape}, uint8_shape: {uint8_shape}, numel: {meta['numel']}, bytes: {meta['param_bytes']}\")\n",
    "\n",
    "    size_data['param_names'].append(param_name)\n",
    "    size_data['numel'].append(meta['numel'])\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    xor_bytes = zstd.decompress(compressed_xor)\n",
    "    xor_arr = np.frombuffer(xor_bytes, dtype=np.uint8).reshape(uint8_shape)\n",
    "    t_decompress = perf_counter() - t0\n",
    "    print(f\"  Decompressed XOR bytes in {t_decompress:.7f}s, shape: {xor_arr.shape}\")\n",
    "\n",
    "    if is_mha:\n",
    "        if 'in_proj_weight' in param_name:\n",
    "            base_data = module.in_proj_weight.data.cpu().numpy()\n",
    "        else:\n",
    "            base_data = module.out_proj_weight.data.cpu().numpy()\n",
    "    else:\n",
    "        base_data = module.weight.data.cpu().numpy()\n",
    "    print(f\"  Base data shape: {base_data.shape}, dtype: {base_data.dtype}\")\n",
    "\n",
    "    base_bytes = base_data.view(np.uint8).reshape(uint8_shape)\n",
    "    ft_bytes = np.bitwise_xor(base_bytes, xor_arr)\n",
    "    new_data = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(shape)\n",
    "    new_data = torch.from_numpy(new_data).to(dtype=torch.float32, device=device)\n",
    "    print(f\"  Finetuned data shape: {new_data.shape}, sample values: {new_data.flatten()[:5]}\")\n",
    "\n",
    "    track_timing(timing_data, tile_data, param_name, t_decompress)\n",
    "\n",
    "    if is_mha and 'in_proj_weight' in param_name:\n",
    "        print(f\"  DEBUG: is_mha=True for {param_name} - updating in-place, NO HOOK\")\n",
    "        module.in_proj_weight.data = new_data.to(module.in_proj_weight.device, module.in_proj_weight.dtype)\n",
    "        return None\n",
    "    \n",
    "    def hook_fn(module, inputs, output):\n",
    "        print(f\"  Applying hook for {param_name} (shaped: {new_data.shape}), input shape: {inputs[0].shape}\")\n",
    "        input_shape = inputs[0].shape\n",
    "        if len(input_shape) == 3:\n",
    "            batch, seq, in_dim = input_shape\n",
    "            x_in = inputs[0].view(batch * seq, in_dim)  # Flatten: [4, 16, 64] -> [64, 64]\n",
    "        else:\n",
    "            x_in = inputs[0]  # Already [batch*seq, in_dim]\n",
    "        y = F.linear(x_in, new_data, module.bias if hasattr(module, 'bias') and module.bias is not None else None)\n",
    "        print(f\"  Applied full matrix multiplication (output = F.linear(input, finetuned weight, bias))\")\n",
    "        if len(input_shape) == 3:\n",
    "            y = y.view(batch, seq, new_data.shape[0])  # Reshape: [64, out_features] -> [4, 16, out_features]\n",
    "        print(f\"  Output shape after hook: {y.shape}\")\n",
    "        return y if not is_mha else (y, output[1])\n",
    "\n",
    "    print(f\"  Created and returning hook function for {param_name}\")\n",
    "    return hook_fn\n",
    "\n",
    "def create_tiled_hook(param_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=False, is_norm=False, is_mha=False):\n",
    "    \"\"\"Dispatch to bias/norm or weight hook creation.\"\"\"\n",
    "    if is_bias or is_norm:\n",
    "        return create_bias_norm_hook(param_name, module, pkg, device, timing_data, size_data, tile_data, is_bias, is_norm, is_mha)\n",
    "    else:\n",
    "        return create_weight_hook(param_name, module, pkg, device, timing_data, size_data, tile_data, is_mha, tile_rows=tile_rows, tile_cols=tile_cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION: Visualize metrics # -----------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "def visualize_metrics(timing_data, size_data, tile_data):\n",
    "    if timing_data['param_names']:\n",
    "        timing_chart = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": timing_data['param_names'],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Decompress Time (s)\",\n",
    "                        \"data\": timing_data['t_partial_decompress'],\n",
    "                        \"backgroundColor\": \"rgba(75, 192, 192, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(75, 192, 192, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"XOR Patch Time (s)\",\n",
    "                        \"data\": timing_data['t_tile_xor_patch'],\n",
    "                        \"backgroundColor\": \"rgba(255, 99, 132, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(255, 99, 132, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Forward Tile Time (s)\",\n",
    "                        \"data\": timing_data['t_forward_tile'],\n",
    "                        \"backgroundColor\": \"rgba(54, 162, 235, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(54, 162, 235, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": \"Timing Breakdown per Parameter\"\n",
    "                    }\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"beginAtZero\": True,\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Time (seconds)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Parameter\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(\"\\nTiming Chart JSON:\")\n",
    "        print(json.dumps(timing_chart, indent=2))\n",
    "\n",
    "        size_chart = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": size_data['param_names'],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Number of Elements\",\n",
    "                        \"data\": size_data['numel'],\n",
    "                        \"backgroundColor\": \"rgba(153, 102, 255, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(153, 102, 255, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": \"Parameter Sizes (Number of Elements)\"\n",
    "                    }\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"beginAtZero\": True,\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Number of Elements\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Parameter\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(\"\\nSize Chart JSON:\")\n",
    "        print(json.dumps(size_chart, indent=2))\n",
    "\n",
    "        tile_chart = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": tile_data['param_names'],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Number of Tiles\",\n",
    "                        \"data\": tile_data['num_tiles'],\n",
    "                        \"backgroundColor\": \"rgba(255, 159, 64, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(255, 159, 64, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": \"Number of Tiles per Parameter\"\n",
    "                    }\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"beginAtZero\": True,\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Number of Tiles\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Parameter\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(\"\\nTile Chart JSON:\")\n",
    "        print(json.dumps(tile_chart, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1 - Starting Function ===\n",
      "Input x shape: torch.Size([4, 16, 64]), device: cpu, tile_rows: 64, tile_cols: 64, test_mode: False\n",
      "Package load time: 0.0000185s\n",
      "pkg keys: ['meta', 'deltas', 'zstd_level', 'stats']\n",
      "Model param names: ['ln1.weight', 'ln1.bias', 'self_attn.in_proj_weight', 'self_attn.in_proj_bias', 'self_attn.out_proj.weight', 'self_attn.out_proj.bias', 'ln2.weight', 'ln2.bias', 'ff.0.weight', 'ff.0.bias', 'ff.2.weight', 'ff.2.bias']\n",
      "Tiny layer params (numel): [64, 64, 12288, 192, 4096, 64, 64, 64, 8192, 128, 8192, 64]\n",
      "Total parameters: 33472\n",
      "Model param shapes:\n",
      "  ln1.weight: torch.Size([64])\n",
      "  ln1.bias: torch.Size([64])\n",
      "  self_attn.in_proj_weight: torch.Size([192, 64])\n",
      "  self_attn.in_proj_bias: torch.Size([192])\n",
      "  self_attn.out_proj.weight: torch.Size([64, 64])\n",
      "  self_attn.out_proj.bias: torch.Size([64])\n",
      "  ln2.weight: torch.Size([64])\n",
      "  ln2.bias: torch.Size([64])\n",
      "  ff.0.weight: torch.Size([128, 64])\n",
      "  ff.0.bias: torch.Size([128])\n",
      "  ff.2.weight: torch.Size([64, 128])\n",
      "  ff.2.bias: torch.Size([64])\n",
      "Delta keys (updates): {'ln1.weight', 'ff.2.weight', 'ff.2.bias', 'ln2.weight', 'self_attn.out_proj.bias', 'ln1.bias', 'ff.0.weight', 'self_attn.out_proj.weight', 'self_attn.in_proj_weight', 'ff.0.bias', 'self_attn.in_proj_bias', 'ln2.bias'}\n",
      "Param keys (model): {'ln1.weight', 'ff.2.weight', 'ff.2.bias', 'ln2.weight', 'self_attn.out_proj.bias', 'ln1.bias', 'ff.0.weight', 'self_attn.out_proj.weight', 'self_attn.in_proj_weight', 'ff.0.bias', 'self_attn.in_proj_bias', 'ln2.bias'}\n",
      "\n",
      "=== Step 2 - Module Collection ===\n",
      "Linear layers: ['self_attn.out_proj', 'ff.0', 'ff.2']\n",
      "Norm layers: ['ln1', 'ln2']\n",
      "MHA layers: ['self_attn']\n",
      "\n",
      "=== Step 3 - Initialize Visualization Data Structures ===\n",
      "Initialized tracking data structures:\n",
      "  Timing keys: ['param_names', 't_partial_decompress', 't_tile_xor_patch', 't_forward_tile']\n",
      "  Size keys: ['param_names', 'numel']\n",
      "  Tile keys: ['param_names', 'num_tiles']\n",
      "\n",
      "=== Registering hooks for linear layers ===\n",
      "  Processing linear layer 'self_attn.out_proj': weight='self_attn.out_proj.weight', bias='self_attn.out_proj.bias'\n",
      "-----------------self_attn.out_proj.weight Hook (Weight)-----------------\n",
      "  Flags: is_mha=False, is_bias=False, is_norm=False\n",
      "  Shape: [64, 64], uint8_shape: [64, 256], numel: 4096, bytes: 16384\n",
      "  Decompressed XOR bytes in 0.0001070s, shape: (64, 256)\n",
      "  Base data shape: (64, 64), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64, 64]), sample values: tensor([ 0.0956,  0.1038, -0.0293,  0.1148, -0.0274])\n",
      "  Created and returning hook function for self_attn.out_proj.weight\n",
      "-----------------self_attn.out_proj.bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=True, is_norm=False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000105s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: Creating forward hook for non-MHA self_attn.out_proj.bias\n",
      "  Created and returning hook function for self_attn.out_proj.bias\n",
      "    Registered weight hook for 'self_attn.out_proj.weight'\n",
      "    Registered bias hook for 'self_attn.out_proj.bias'\n",
      "  Processing linear layer 'ff.0': weight='ff.0.weight', bias='ff.0.bias'\n",
      "-----------------ff.0.weight Hook (Weight)-----------------\n",
      "  Flags: is_mha=False, is_bias=False, is_norm=False\n",
      "  Shape: [128, 64], uint8_shape: [128, 256], numel: 8192, bytes: 32768\n",
      "  Decompressed XOR bytes in 0.0000174s, shape: (128, 256)\n",
      "  Base data shape: (128, 64), dtype: float32\n",
      "  Finetuned data shape: torch.Size([128, 64]), sample values: tensor([ 0.0183, -0.1063, -0.0856,  0.1007, -0.1142])\n",
      "  Created and returning hook function for ff.0.weight\n",
      "-----------------ff.0.bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=True, is_norm=False\n",
      "  Shape: [128], uint8_shape: [512], numel: 128, bytes: 512\n",
      "  Decompressed XOR bytes in 0.0000049s, shape: (512,)\n",
      "  Base data shape: (128,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([128]), sample values: tensor([ 0.0040, -0.0448, -0.0866,  0.0217,  0.1057])\n",
      "  DEBUG: Creating forward hook for non-MHA ff.0.bias\n",
      "  Created and returning hook function for ff.0.bias\n",
      "    Registered weight hook for 'ff.0.weight'\n",
      "    Registered bias hook for 'ff.0.bias'\n",
      "  Processing linear layer 'ff.2': weight='ff.2.weight', bias='ff.2.bias'\n",
      "-----------------ff.2.weight Hook (Weight)-----------------\n",
      "  Flags: is_mha=False, is_bias=False, is_norm=False\n",
      "  Shape: [64, 128], uint8_shape: [64, 512], numel: 8192, bytes: 32768\n",
      "  Decompressed XOR bytes in 0.0000136s, shape: (64, 512)\n",
      "  Base data shape: (64, 128), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64, 128]), sample values: tensor([ 0.0465,  0.0254,  0.0250, -0.0816, -0.0068])\n",
      "  Created and returning hook function for ff.2.weight\n",
      "-----------------ff.2.bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=True, is_norm=False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000047s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([-0.0282,  0.0206, -0.0500, -0.0040, -0.0768])\n",
      "  DEBUG: Creating forward hook for non-MHA ff.2.bias\n",
      "  Created and returning hook function for ff.2.bias\n",
      "    Registered weight hook for 'ff.2.weight'\n",
      "    Registered bias hook for 'ff.2.bias'\n",
      "  Total linear hooks registered: 6\n",
      "\n",
      "=== Registering hooks for norm layers ===\n",
      "  Processing norm layer 'ln1': weight='ln1.weight', bias='ln1.bias'\n",
      "-----------------ln1.weight Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=False, is_norm=True\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000040s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([1., 1., 1., 1., 1.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln1.weight\n",
      "  Created and returning hook function for ln1.weight\n",
      "-----------------ln1.bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=True, is_norm=False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000054s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln1.bias\n",
      "  Created and returning hook function for ln1.bias\n",
      "    Registered weight hook for 'ln1.weight'\n",
      "    Registered bias hook for 'ln1.bias'\n",
      "  Processing norm layer 'ln2': weight='ln2.weight', bias='ln2.bias'\n",
      "-----------------ln2.weight Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=False, is_norm=True\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000042s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([1., 1., 1., 1., 1.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln2.weight\n",
      "  Created and returning hook function for ln2.weight\n",
      "-----------------ln2.bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=False, is_bias=True, is_norm=False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000033s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln2.bias\n",
      "  Created and returning hook function for ln2.bias\n",
      "    Registered weight hook for 'ln2.weight'\n",
      "    Registered bias hook for 'ln2.bias'\n",
      "  Processed 6 linear parameters, 4 norm parameters (total attempts: 10)\n",
      "  Actual hooks registered so far: 10 (placeholders return None)\n",
      "\n",
      "=== Registering hooks for MHA layers ===\n",
      "  Processing MHA layer 'self_attn':\n",
      "    Processing weight 'self_attn.in_proj_weight'\n",
      "-----------------self_attn.in_proj_weight Hook (Weight)-----------------\n",
      "  Flags: is_mha=True, is_bias=False, is_norm=False\n",
      "  Shape: [192, 64], uint8_shape: [192, 256], numel: 12288, bytes: 49152\n",
      "  Decompressed XOR bytes in 0.0000184s, shape: (192, 256)\n",
      "  Base data shape: (192, 64), dtype: float32\n",
      "  Finetuned data shape: torch.Size([192, 64]), sample values: tensor([ 0.1393, -0.0173,  0.0589, -0.1216,  0.0816])\n",
      "  DEBUG: is_mha=True for self_attn.in_proj_weight - updating in-place, NO HOOK\n",
      "    Processing weight 'self_attn.out_proj_weight'\n",
      "-----------------self_attn.out_proj_weight Hook (Weight)-----------------\n",
      "  Flags: is_mha=True, is_bias=False, is_norm=False\n",
      "  Skipping self_attn.out_proj_weight: no update in pkg['deltas']\n",
      "    Processing bias 'self_attn.in_proj_bias'\n",
      "-----------------self_attn.in_proj_bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=True, is_bias=True, is_norm=False\n",
      "  Shape: [192], uint8_shape: [768], numel: 192, bytes: 768\n",
      "  Decompressed XOR bytes in 0.0000035s, shape: (768,)\n",
      "  Base data shape: (192,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([192]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: is_mha=True, is_bias=True for self_attn.in_proj_bias - updating in-place, NO HOOK\n",
      "  Updated MHA bias parameter in-place for self_attn.in_proj_bias\n",
      "    Processing bias 'self_attn.out_proj_bias'\n",
      "-----------------self_attn.out_proj_bias Hook (Bias/Norm)-----------------\n",
      "  Flags: is_mha=True, is_bias=True, is_norm=False\n",
      "  Skipping self_attn.out_proj_bias: no update in pkg['deltas']\n",
      "  Processed 4 MHA parameters (total attempts: 14)\n",
      "  Actual hooks registered so far: 10 (placeholders return None)\n",
      "\n",
      "=== Running forward pass with hooks ===\n",
      "  Applying hook for ln1.weight (shaped: torch.Size([64])), input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias (shaped: torch.Size([64])), input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln2.weight (shaped: torch.Size([64])), input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln2.bias (shaped: torch.Size([64])), input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ff.0.weight (shaped: torch.Size([128, 64])), input shape: torch.Size([4, 16, 64])\n",
      "  Applied full matrix multiplication (output = F.linear(input, finetuned weight, bias))\n",
      "  Output shape after hook: torch.Size([4, 16, 128])\n",
      "  Applying hook for ff.0.bias (shaped: torch.Size([128])), input shape: torch.Size([4, 16, 64])\n",
      "  Applied bias addition (expanded to torch.Size([4, 16, 128]))\n",
      "  Output shape after hook: torch.Size([4, 16, 128])\n",
      "  Applying hook for ff.2.weight (shaped: torch.Size([64, 128])), input shape: torch.Size([4, 16, 128])\n",
      "  Applied full matrix multiplication (output = F.linear(input, finetuned weight, bias))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ff.2.bias (shaped: torch.Size([64])), input shape: torch.Size([4, 16, 128])\n",
      "  Applied bias addition (expanded to torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "Forward pass time: 0.0039342s\n",
      "Total time: 0.0039527s (load: 0.0000185s, forward: 0.0039342s)\n",
      "Final output shape: torch.Size([4, 16, 64])\n",
      "\n",
      "=== Cleaning up hooks ===\n",
      "Removed 10 hook handles\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TODO: compute on decompressed XOR without global XOR\n",
    "# -----------------------------\n",
    "def e2e_forward_on_delta_no_full_reconstruct_TODO(base_layer: nn.Module, pkg_bytes: bytes, x: torch.Tensor,\n",
    "                                                  *, device: str = 'cpu', tile_rows: int = 64, tile_cols: int = 64,\n",
    "                                                  test_mode: bool = False):\n",
    "    print(f\"\\n=== Step 1 - Starting Function ===\")\n",
    "    print(f\"Input x shape: {x.shape}, device: {device}, tile_rows: {tile_rows}, tile_cols: {tile_cols}, test_mode: {test_mode}\")\n",
    "    t0 = perf_counter()\n",
    "    pkg = pickle.loads(pkg_bytes)\n",
    "    t_load = perf_counter() - t0\n",
    "    print(f\"Package load time: {t_load:.7f}s\")\n",
    "    print(f\"pkg keys: {list(pkg.keys())}\")\n",
    "    print(f\"Model param names: {[name for name, _ in base_layer.named_parameters()]}\")\n",
    "    print(f\"Tiny layer params (numel): {[p.numel() for p in base_layer.parameters()]}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in base_layer.parameters())}\")\n",
    "\n",
    "    print(f\"Model param shapes:\")\n",
    "    for name, p in base_layer.named_parameters():\n",
    "        print(f\"  {name}: {p.shape}\")\n",
    "\n",
    "    delta_keys = set(pkg['deltas'].keys())\n",
    "    param_keys = set(name for name, _ in base_layer.named_parameters())\n",
    "    print(f\"Delta keys (updates): {delta_keys}\")\n",
    "    print(f\"Param keys (model): {param_keys}\")\n",
    "    if not delta_keys.issubset(param_keys):\n",
    "        print(f\"Warning: pkg['deltas'] contains extra keys: {delta_keys - param_keys}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Step 2 - Module Collection ===\")\n",
    "    named_linears = [(name, module) for name, module in base_layer.named_modules() if isinstance(module, nn.Linear)]\n",
    "    named_norms = [(name, module) for name, module in base_layer.named_modules() if isinstance(module, nn.LayerNorm)]\n",
    "    named_mha = [(name, module) for name, module in base_layer.named_modules() if isinstance(module, nn.MultiheadAttention)]\n",
    "    print(f\"Linear layers: {[name for name, _ in named_linears]}\")\n",
    "    print(f\"Norm layers: {[name for name, _ in named_norms]}\")\n",
    "    print(f\"MHA layers: {[name for name, _ in named_mha]}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Step 3 - Initialize Visualization Data Structures ===\")\n",
    "    timing_data = {'param_names': [], 't_partial_decompress': [], 't_tile_xor_patch': [], 't_forward_tile': []}\n",
    "    size_data = {'param_names': [], 'numel': []}\n",
    "    tile_data = {'param_names': [], 'num_tiles': []}\n",
    "    print(f\"Initialized tracking data structures:\")\n",
    "    print(f\"  Timing keys: {list(timing_data.keys())}\")\n",
    "    print(f\"  Size keys: {list(size_data.keys())}\")\n",
    "    print(f\"  Tile keys: {list(tile_data.keys())}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n=== Registering hooks for linear layers ===\")\n",
    "    hook_handles = []\n",
    "    for name, module in named_linears:\n",
    "        weight_name = name + '.weight' if name else 'weight'\n",
    "        bias_name = name + '.bias' if name else 'bias'\n",
    "        print(f\"  Processing linear layer '{name}': weight='{weight_name}', bias='{bias_name}'\")\n",
    "        weight_hook = create_tiled_hook(weight_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data)\n",
    "        bias_hook = create_tiled_hook(bias_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=True)\n",
    "        if weight_hook:\n",
    "            hook_handles.append(module.register_forward_hook(weight_hook))\n",
    "            print(f\"    Registered weight hook for '{weight_name}'\")\n",
    "        if bias_hook:\n",
    "            hook_handles.append(module.register_forward_hook(bias_hook))\n",
    "            print(f\"    Registered bias hook for '{bias_name}'\")\n",
    "    print(f\"  Total linear hooks registered: {len(hook_handles)}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n=== Registering hooks for norm layers ===\")\n",
    "    linear_attempts = len(named_linears) * 2  # weight + bias per linear\n",
    "    norm_attempts = 0\n",
    "    for name, module in named_norms:\n",
    "        w_name = name + '.weight' if name else 'weight'\n",
    "        b_name = name + '.bias' if name else 'bias'\n",
    "        print(f\"  Processing norm layer '{name}': weight='{w_name}', bias='{b_name}'\")\n",
    "        w_hook = create_tiled_hook(w_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_norm=True)\n",
    "        b_hook = create_tiled_hook(b_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=True)\n",
    "        if w_hook:\n",
    "            hook_handles.append(module.register_forward_hook(w_hook))\n",
    "            print(f\"    Registered weight hook for '{w_name}'\")\n",
    "        if b_hook:\n",
    "            hook_handles.append(module.register_forward_hook(b_hook))\n",
    "            print(f\"    Registered bias hook for '{b_name}'\")\n",
    "        norm_attempts += 2  # weight + bias attempt\n",
    "    print(f\"  Processed {linear_attempts} linear parameters, {norm_attempts} norm parameters (total attempts: {linear_attempts + norm_attempts})\")\n",
    "    print(f\"  Actual hooks registered so far: {len(hook_handles)} (placeholders return None)\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Registering hooks for MHA layers ===\")\n",
    "    mha_attempts = 0\n",
    "    for name, module in named_mha:\n",
    "        print(f\"  Processing MHA layer '{name}':\")\n",
    "        # Weights\n",
    "        for w_name in ['in_proj_weight', 'out_proj_weight']:\n",
    "            full_name = name + '.' + w_name if name else w_name\n",
    "            print(f\"    Processing weight '{full_name}'\")\n",
    "            hook = create_tiled_hook(full_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_mha=True)\n",
    "            if hook:\n",
    "                hook_handles.append(module.register_forward_hook(hook))\n",
    "                print(f\"      Registered weight hook for '{full_name}'\")\n",
    "        # Biases\n",
    "        for b_name in ['in_proj_bias', 'out_proj_bias']:\n",
    "            full_name = name + '.' + b_name if name else b_name\n",
    "            print(f\"    Processing bias '{full_name}'\")\n",
    "            hook = create_tiled_hook(full_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=True, is_mha=True)\n",
    "            if hook:\n",
    "                hook_handles.append(module.register_forward_hook(hook))\n",
    "                print(f\"      Registered bias hook for '{full_name}'\")\n",
    "        mha_attempts += 4  # 2 weights + 2 biases per MHA\n",
    "    print(f\"  Processed {mha_attempts} MHA parameters (total attempts: {linear_attempts + norm_attempts + mha_attempts})\")\n",
    "    print(f\"  Actual hooks registered so far: {len(hook_handles)} (placeholders return None)\")\n",
    "\n",
    "    print(f\"\\n=== Running forward pass with hooks ===\")\n",
    "    t0 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        output = base_layer(x.to(device))\n",
    "    t_total = perf_counter() - t0\n",
    "    print(f\"Forward pass time: {t_total:.7f}s\")\n",
    "    print(f\"Total time: {t_total + t_load:.7f}s (load: {t_load:.7f}s, forward: {t_total:.7f}s)\")\n",
    "    print(f\"Final output shape: {output.shape if not isinstance(output, tuple) else output[0].shape}\")\n",
    "\n",
    "    print(f\"\\n=== Cleaning up hooks ===\")\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    print(f\"Removed {len(hook_handles)} hook handles\")\n",
    "\n",
    "\n",
    "x_bench = torch.randn(4, 16, 64)\n",
    "temp = e2e_forward_on_delta_no_full_reconstruct_TODO(base_layer, pkg_bytes, x_bench, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
