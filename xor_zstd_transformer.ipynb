{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Tiny layer params: 33472\n",
      "Changed elements: 994 / 33472 (~2.97%)\n",
      "=== Compression Stats ===\n",
      "Total param bytes  : 133888\n",
      "Total delta bytes  : 133888\n",
      "Total compressed   : 5633\n",
      "Global ratio       : 0.04207247848948375\n",
      "- ln1.weight                               | param=256 | comp=27 | ratio=0.1055\n",
      "- ln1.bias                                 | param=256 | comp=26 | ratio=0.1016\n",
      "- self_attn.in_proj_weight                 | param=49152 | comp=1963 | ratio=0.0399\n",
      "- self_attn.in_proj_bias                   | param=768 | comp=53 | ratio=0.0690\n",
      "- self_attn.out_proj.weight                | param=16384 | comp=707 | ratio=0.0432\n",
      "=== Component-wise Timing (seconds): mean ± std over 7 runs ===\n",
      "    t_load_pkg: 0.000016 ± 0.000003\n",
      "  t_decompress: 0.000050 ± 0.000015\n",
      "   t_xor_apply: 0.000053 ± 0.000013\n",
      "  t_state_load: 0.000938 ± 0.001317\n",
      "     t_forward: 0.012634 ± 0.030183\n",
      "  t_end_to_end: 0.013692 ± 0.031508\n",
      "Sanity checksums (first 3): [3409.4951171875, 3409.4951171875, 3409.4951171875]\n",
      "=== What-if Speed Windows (vs current E2E mean) ===\n",
      "Baseline E2E: 0.013692s\n",
      "Skip XOR (keep decompress): 0.013639s  → 1.00× faster (↓0.4%)\n",
      "Skip XOR+Decompress (Zstd-domain compute): 0.013589s  → 1.01× faster (↓0.8%)\n",
      "\n",
      "Interpretation:\n",
      "- (Baseline → Bound A) is the best-case improvement if we avoid the global XOR-apply step by\n",
      "  computing directly on decompressed XOR bytes (tile JIT patching, patch-based kernels, etc.).\n",
      "- (Baseline → Bound B) is a theoretical ceiling if we could avoid both decompression and XOR\n",
      "  (i.e., compute in Zstd domain); not realistic today, but shows maximum headroom.\n",
      "=== Top 5 params by XOR-apply time ===\n",
      "ln1.weight                               | bytes=     256 | decompress=0.000006s | xor=0.000128s\n",
      "self_attn.in_proj_weight                 | bytes=   49152 | decompress=0.000013s | xor=0.000016s\n",
      "ff.0.weight                              | bytes=   32768 | decompress=0.000007s | xor=0.000011s\n",
      "ff.2.weight                              | bytes=   32768 | decompress=0.000007s | xor=0.000010s\n",
      "self_attn.out_proj.weight                | bytes=   16384 | decompress=0.000003s | xor=0.000007s\n",
      "\n",
      "=== XOR Sparsity Estimate ===\n",
      "xor_total_bytes             : 133888\n",
      "xor_nonzero_bytes           : 3138\n",
      "nonzero_density             : 0.0234375\n",
      "dense_storage_bytes         : 133888\n",
      "simple_sparse_storage_bytes : 15690\n",
      "sparse_over_dense_ratio     : 0.1171875\n",
      "\n",
      "NOTE: If non-zero density is low, consider block-sparse indexing (e.g., 256B/4KB blocks), RLE on zero-runs, or bitplane packing so **many XOR adapters can co-reside on a single GPU.**\n"
     ]
    }
   ],
   "source": [
    "# Given naive approach benchmarks\n",
    "# If needed, install deps:\n",
    "# !pip install torch zstandard\n",
    "\n",
    "import math, io, pickle, time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    import zstandard as zstd\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"Please install `zstandard` (pip install zstandard) before running this cell.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny Transformer layer\n",
    "# -----------------------------\n",
    "class TinyTransformerLayer(nn.Module):\n",
    "    \"\"\"Small block with MHA + FFN, sufficient for demonstration.\"\"\"\n",
    "    def __init__(self, d_model=64, nhead=4, dim_feedforward=128, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.self_attn(h, h, h, attn_mask=attn_mask)\n",
    "        x = x + a\n",
    "        h2 = self.ln2(x)\n",
    "        y = self.ff(h2)\n",
    "        return x + y\n",
    "\n",
    "base_layer = TinyTransformerLayer().eval()\n",
    "print(\"Tiny layer params:\", sum(p.numel() for p in base_layer.parameters()))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simulate finetuning (sparse updates)\n",
    "# -----------------------------\n",
    "from copy import deepcopy\n",
    "\n",
    "def perturb_subset_(module: nn.Module, frac: float = 0.03, magnitude: float = 1e-2, seed: int = 0):\n",
    "    \"\"\"In-place: perturb `frac` of elements in each float parameter.\n",
    "    This simulates sparse finetuning, making XOR(base, finetuned) mostly zeros.\n",
    "    \"\"\"\n",
    "    g = torch.Generator(device='cpu').manual_seed(seed)\n",
    "    with torch.no_grad():\n",
    "        for name, p in module.named_parameters():\n",
    "            if not p.dtype.is_floating_point:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            k = max(1, int(numel * frac))\n",
    "            idx = torch.randperm(numel, generator=g, device=p.device)[:k]\n",
    "            flat = p.view(-1)\n",
    "            noise = magnitude * torch.randn(k, generator=g, device=p.device, dtype=flat.dtype)\n",
    "            flat[idx] += noise\n",
    "\n",
    "finetuned_layer = deepcopy(base_layer)\n",
    "perturb_subset_(finetuned_layer, frac=0.03, magnitude=1e-2, seed=123)\n",
    "\n",
    "# Count changed elements (exact because we add noise)\n",
    "changed, total = 0, 0\n",
    "for (n1, p_base), (n2, p_ft) in zip(base_layer.named_parameters(), finetuned_layer.named_parameters()):\n",
    "    assert n1 == n2\n",
    "    if p_base.dtype.is_floating_point:\n",
    "        diff = (p_base != p_ft).sum().item()\n",
    "        changed += diff\n",
    "        total += p_base.numel()\n",
    "print(f\"Changed elements: {changed} / {total} (~{changed/total*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR + Zstd compression helpers\n",
    "# -----------------------------\n",
    "def tensor_to_uint8_view(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Return a NumPy uint8 *view* over tensor bytes (CPU).\"\"\"\n",
    "    a = t.detach().cpu().numpy()\n",
    "    return a.view(np.uint8)\n",
    "\n",
    "def xor_uint8(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    assert a.dtype == np.uint8 and b.dtype == np.uint8 and a.size == b.size\n",
    "    return np.bitwise_xor(a, b)\n",
    "\n",
    "def compress_state_dict_with_xor_zstd(base_sd: dict, finetuned_sd: dict, level: int = 10):\n",
    "    \"\"\"Pack XOR(Zstd) deltas.\n",
    "    Returns a dict with:\n",
    "      - meta[name]: shape/dtype/numel/param_bytes/uint8_shape\n",
    "      - deltas[name]: Zstd-compressed XOR bytes\n",
    "      - stats: aggregate sizes\n",
    "    \"\"\"\n",
    "    compressor = zstd.ZstdCompressor(level=level)\n",
    "    meta, deltas = {}, {}\n",
    "    stats = {'total_params_bytes': 0, 'total_delta_bytes': 0, 'total_compressed_bytes': 0, 'per_param': {}}\n",
    "\n",
    "    for name, base_t in base_sd.items():\n",
    "        ft_t = finetuned_sd[name]\n",
    "        base_bytes = tensor_to_uint8_view(base_t)\n",
    "        ft_bytes = tensor_to_uint8_view(ft_t)\n",
    "        assert base_bytes.size == ft_bytes.size, name\n",
    "\n",
    "        delta_u8 = xor_uint8(base_bytes, ft_bytes)\n",
    "        raw_delta = delta_u8.tobytes()\n",
    "        comp = compressor.compress(raw_delta)\n",
    "\n",
    "        meta[name] = {\n",
    "            'shape': list(ft_t.shape),\n",
    "            'dtype_str': str(ft_t.detach().cpu().numpy().dtype.str),  # e.g. '<f4'\n",
    "            'numel': ft_t.numel(),\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'uint8_shape': list(base_bytes.shape),\n",
    "        }\n",
    "        deltas[name] = comp\n",
    "\n",
    "        stats['total_params_bytes'] += ft_bytes.size\n",
    "        stats['total_delta_bytes'] += len(raw_delta)\n",
    "        stats['total_compressed_bytes'] += len(comp)\n",
    "        stats['per_param'][name] = {\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'delta_bytes': len(raw_delta),\n",
    "            'compressed_bytes': len(comp),\n",
    "            'compressed_ratio_vs_param': len(comp) / max(1, ft_bytes.size),\n",
    "        }\n",
    "\n",
    "    return {'meta': meta, 'deltas': deltas, 'zstd_level': level, 'stats': stats}\n",
    "\n",
    "def reconstruct_finetuned_state_dict_with_timing(base_sd: dict, pkg: dict):\n",
    "    \"\"\"Decompress and XOR-apply with timing breakdown.\n",
    "    Returns: (rec_sd, timing_dict) with keys: t_decompress, t_xor_apply\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    rec = {}\n",
    "    t_decomp = 0.0\n",
    "    t_xor = 0.0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        # Decompress\n",
    "        t0 = perf_counter()\n",
    "        raw_delta = dctx.decompress(comp)\n",
    "        t_decomp += perf_counter() - t0\n",
    "        delta_u8 = np.frombuffer(raw_delta, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "\n",
    "        # XOR apply\n",
    "        base_bytes = tensor_to_uint8_view(base_sd[name])\n",
    "        t1 = perf_counter()\n",
    "        ft_bytes = np.bitwise_xor(base_bytes, delta_u8)  # shape-aligned XOR\n",
    "        t_xor += perf_counter() - t1\n",
    "\n",
    "        # Reinterpret as original dtype/shape\n",
    "        arr = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(meta['shape']).copy()\n",
    "        rec[name] = torch.from_numpy(arr)\n",
    "    return rec, {'t_decompress': t_decomp, 't_xor_apply': t_xor}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build base/finetuned SDs and compress\n",
    "# -----------------------------\n",
    "base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "ft_sd   = {k: v.detach().cpu().contiguous() for k, v in finetuned_layer.state_dict().items()}\n",
    "\n",
    "pkg = compress_state_dict_with_xor_zstd(base_sd, ft_sd, level=10)\n",
    "print(\"=== Compression Stats ===\")\n",
    "print(\"Total param bytes  :\", pkg['stats']['total_params_bytes'])\n",
    "print(\"Total delta bytes  :\", pkg['stats']['total_delta_bytes'])\n",
    "print(\"Total compressed   :\", pkg['stats']['total_compressed_bytes'])\n",
    "print(\"Global ratio       :\", pkg['stats']['total_compressed_bytes']/pkg['stats']['total_params_bytes'])\n",
    "for name, st in list(pkg['stats']['per_param'].items())[:5]:\n",
    "    print(f\"- {name:40s} | param={st['param_bytes']} | comp={st['compressed_bytes']} | ratio={st['compressed_ratio_vs_param']:.4f}\")\n",
    "\n",
    "# Serialize to in-memory bytes (so we can measure \"load\" as deserialization;\n",
    "# replace with actual disk I/O in your system if you want true file load timings).\n",
    "pkg_bytes = pickle.dumps(pkg)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end function with timers (load → reconstruct → load_state_dict → forward)\n",
    "# -----------------------------\n",
    "def e2e_forward_from_pkg_bytes(base_layer: nn.Module, pkg_bytes: bytes, x: torch.Tensor, device: str = 'cpu'):\n",
    "    \"\"\"End-to-end: load (deserialize) → decompress → XOR-apply → load_state_dict → forward.\n",
    "    Returns (timings_dict, checksum).\n",
    "    NOTE: 't_load_pkg' here measures pickle.loads (in-memory). Replace with disk I/O for on-disk timings.\n",
    "    \"\"\"\n",
    "    # 0) Prepare inputs\n",
    "    x = x.to(device)\n",
    "    base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "\n",
    "    # 1) Load package (deserialize)\n",
    "    t0 = perf_counter()\n",
    "    pkg = pickle.loads(pkg_bytes)\n",
    "    t_load = perf_counter() - t0\n",
    "\n",
    "    # 2) Decompress + XOR apply (split timings)\n",
    "    rec_sd, t_parts = reconstruct_finetuned_state_dict_with_timing(base_sd, pkg)\n",
    "\n",
    "    # 3) Materialize a new layer and copy weights\n",
    "    t2 = perf_counter()\n",
    "    rec_layer = TinyTransformerLayer().to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for k, p in rec_layer.state_dict().items():\n",
    "            p.copy_(rec_sd[k].to(device))\n",
    "    t_state = perf_counter() - t2\n",
    "\n",
    "    # 4) Forward once (demo)\n",
    "    t3 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        y = rec_layer(x)\n",
    "    t_fwd = perf_counter() - t3\n",
    "\n",
    "    timings = {\n",
    "        't_load_pkg': t_load,\n",
    "        't_decompress': t_parts['t_decompress'],\n",
    "        't_xor_apply': t_parts['t_xor_apply'],\n",
    "        't_state_load': t_state,\n",
    "        't_forward': t_fwd,\n",
    "        't_end_to_end': t_load + t_parts['t_decompress'] + t_parts['t_xor_apply'] + t_state + t_fwd,\n",
    "    }\n",
    "    checksum = float(y.abs().sum().detach().cpu())  # simple checksum to ensure consistent output\n",
    "    return timings, checksum\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Micro-benchmark: mean ± std per stage\n",
    "# -----------------------------\n",
    "def benchmark_e2e_breakdown(base_layer, pkg_bytes: bytes, x: torch.Tensor, device: str = 'cpu', repeats: int = 7):\n",
    "    \"\"\"Repeat the E2E pipeline `repeats` times and report mean/std for each stage.\"\"\"\n",
    "    keys = ['t_load_pkg','t_decompress','t_xor_apply','t_state_load','t_forward','t_end_to_end']\n",
    "    buf = {k: [] for k in keys}\n",
    "    checks = []\n",
    "    for _ in range(repeats):\n",
    "        timings, checksum = e2e_forward_from_pkg_bytes(base_layer, pkg_bytes, x, device=device)\n",
    "        for k in keys:\n",
    "            buf[k].append(timings[k])\n",
    "        checks.append(checksum)\n",
    "    stats = {k: {'mean': float(np.mean(v)), 'std': float(np.std(v))} for k, v in buf.items()}\n",
    "    return stats, checks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# What-if speed windows\n",
    "# -----------------------------\n",
    "def print_what_if_bounds(stats):\n",
    "    \"\"\"Compute what-if bounds from measured means:\n",
    "       - Bound A: skip global XOR (still decompress) → compute on decompressed XOR bytes\n",
    "       - Bound B: skip XOR + decompress (theoretical Zstd-domain compute)\n",
    "    \"\"\"\n",
    "    t_load = stats['t_load_pkg']['mean']\n",
    "    t_decomp = stats['t_decompress']['mean']\n",
    "    t_xor = stats['t_xor_apply']['mean']\n",
    "    t_state = stats['t_state_load']['mean']\n",
    "    t_fwd = stats['t_forward']['mean']\n",
    "    t_e2e = stats['t_end_to_end']['mean']\n",
    "\n",
    "    t_boundA = t_load + t_decomp + t_state + t_fwd       # skip XOR\n",
    "    t_boundB = t_load + t_state + t_fwd                  # skip XOR + decompress (theoretical)\n",
    "\n",
    "    def fmt_speedup(old, new):\n",
    "        return f\"{old/new:.2f}× faster (↓{(1 - new/old)*100:.1f}%)\" if new > 0 else \"∞\"\n",
    "\n",
    "    print(\"=== What-if Speed Windows (vs current E2E mean) ===\")\n",
    "    print(f\"Baseline E2E: {t_e2e:.6f}s\")\n",
    "    print(f\"Skip XOR (keep decompress): {t_boundA:.6f}s  → {fmt_speedup(t_e2e, t_boundA)}\")\n",
    "    print(f\"Skip XOR+Decompress (Zstd-domain compute): {t_boundB:.6f}s  → {fmt_speedup(t_e2e, t_boundB)}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- (Baseline → Bound A) is the best-case improvement if we avoid the global XOR-apply step by\")\n",
    "    print(\"  computing directly on decompressed XOR bytes (tile JIT patching, patch-based kernels, etc.).\")\n",
    "    print(\"- (Baseline → Bound B) is a theoretical ceiling if we could avoid both decompression and XOR\")\n",
    "    print(\"  (i.e., compute in Zstd domain); not realistic today, but shows maximum headroom.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Per-parameter hotspots (identify where XOR time concentrates)\n",
    "# -----------------------------\n",
    "def per_param_decompress_xor_timing(base_sd: dict, pkg: dict, topk: int = 5):\n",
    "    \"\"\"Per-parameter timing for decompress and XOR-apply to identify hotspots.\"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    records = []\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        t0 = perf_counter(); raw = dctx.decompress(comp); t_de = perf_counter()-t0\n",
    "        delta = np.frombuffer(raw, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "        t1 = perf_counter(); _ = np.bitwise_xor(tensor_to_uint8_view(base_sd[name]), delta); t_x = perf_counter()-t1\n",
    "        records.append((name, meta['param_bytes'], t_de, t_x))\n",
    "    # sort by XOR time (or total)\n",
    "    records.sort(key=lambda r: r[3], reverse=True)\n",
    "    print(f\"=== Top {topk} params by XOR-apply time ===\")\n",
    "    for name, sz, tde, tx in records[:topk]:\n",
    "        print(f\"{name:40s} | bytes={sz:8d} | decompress={tde:.6f}s | xor={tx:.6f}s\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR sparsity & naive sparse storage estimate\n",
    "# -----------------------------\n",
    "def estimate_xor_sparsity_and_storage(pkg: dict):\n",
    "    \"\"\"Measure XOR sparsity (non-zero bytes) and estimate a simple sparse storage cost.\n",
    "    Model: store (index:uint32 + value:uint8) per non-zero byte.\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    total_bytes = 0\n",
    "    total_nz = 0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        raw = dctx.decompress(pkg['deltas'][name])\n",
    "        arr = np.frombuffer(raw, dtype=np.uint8)\n",
    "        nz = int((arr != 0).sum())\n",
    "        total_bytes += arr.size\n",
    "        total_nz += nz\n",
    "    density = total_nz / max(1, total_bytes)\n",
    "    dense_bytes = total_bytes  # 1 byte per entry\n",
    "    # Sparse: 4 bytes for index + 1 byte value (very simple model)\n",
    "    sparse_bytes = total_nz * (4 + 1)\n",
    "    return {\n",
    "        'xor_total_bytes': total_bytes,\n",
    "        'xor_nonzero_bytes': total_nz,\n",
    "        'nonzero_density': density,\n",
    "        'dense_storage_bytes': dense_bytes,\n",
    "        'simple_sparse_storage_bytes': sparse_bytes,\n",
    "        'sparse_over_dense_ratio': sparse_bytes / max(1, dense_bytes),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run benchmarks\n",
    "# -----------------------------\n",
    "# End-to-end breakdown\n",
    "x_bench = torch.randn(4, 16, 64)\n",
    "stats, checks = benchmark_e2e_breakdown(base_layer, pkg_bytes, x_bench, device=device, repeats=7)\n",
    "print(\"=== Component-wise Timing (seconds): mean ± std over 7 runs ===\")\n",
    "for k, s in stats.items():\n",
    "    print(f\"{k:>14s}: {s['mean']:.6f} ± {s['std']:.6f}\")\n",
    "print(\"Sanity checksums (first 3):\", checks[:3])\n",
    "\n",
    "# What-if bounds\n",
    "print_what_if_bounds(stats)\n",
    "\n",
    "# Hotspots\n",
    "_ = per_param_decompress_xor_timing(base_sd, pkg, topk=5)\n",
    "\n",
    "# XOR sparsity summary\n",
    "sparsity = estimate_xor_sparsity_and_storage(pkg)\n",
    "print(\"\\n=== XOR Sparsity Estimate ===\")\n",
    "for k, v in sparsity.items():\n",
    "    print(f\"{k:28s}: {v}\")\n",
    "print(\"\\nNOTE: If non-zero density is low, consider block-sparse indexing (e.g., 256B/4KB blocks), \"\n",
    "      \"RLE on zero-runs, or bitplane packing so **many XOR adapters can co-reside on a single GPU.**\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMM on zstd Compressed XORed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION: Tiled Hooks # -----------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def update_mha_bias_in_place(module, param_name, new_data, device):\n",
    "    \"\"\"Update the MHA bias parameter in-place with finetuned data.\"\"\"\n",
    "    print(f\"  Updated MHA bias parameter in-place for {param_name}\")\n",
    "    if 'in_proj_bias' in param_name:\n",
    "        module.in_proj_bias.data = new_data.to(module.in_proj_bias.device, module.in_proj_bias.dtype)\n",
    "    else:\n",
    "        module.out_proj.bias.data = new_data.to(module.out_proj.bias.device, module.out_proj.bias.dtype)\n",
    "    return None  # No forward hook needed\n",
    "\n",
    "\n",
    "def track_timing(timing_data, tile_data, param_name, t_decompress, t_xor=0.0, t_forward=0.0, num_tiles=1):\n",
    "    \"\"\"Track the timing metrics for this parameter.\"\"\"\n",
    "    timing_data['param_names'].append(param_name)\n",
    "    timing_data['t_partial_decompress'].append(t_decompress)\n",
    "    timing_data['t_tile_xor_patch'].append(t_xor)\n",
    "    timing_data['t_forward_tile'].append(t_forward)\n",
    "    tile_data['param_names'].append(param_name)\n",
    "    tile_data['num_tiles'].append(num_tiles)\n",
    "\n",
    "\n",
    "def create_tiled_hook(param_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=False, is_norm=False, is_mha=False):\n",
    "    \"\"\"Create a forward hook that applies finetuning updates to a parameter.\"\"\"\n",
    "    print(f\"-----------------{param_name} Hook-----------------\")\n",
    "    print(is_mha, is_bias, is_norm)\n",
    "    if param_name not in pkg['deltas']:\n",
    "        print(f\"Skipping {param_name}: no update in pkg['deltas']\")\n",
    "        return None\n",
    "\n",
    "    meta = pkg['meta'][param_name]\n",
    "    shape = meta['shape']\n",
    "    uint8_shape = meta['uint8_shape']\n",
    "    compressed_xor = pkg['deltas'][param_name]\n",
    "    print(f\"  Shape: {shape}, uint8_shape: {uint8_shape}, numel: {meta['numel']}, bytes: {meta['param_bytes']}\")\n",
    "\n",
    "    size_data['param_names'].append(param_name)\n",
    "    size_data['numel'].append(meta['numel'])\n",
    "\n",
    "    # For now, just return None - hook logic comes in later steps\n",
    "    t0 = perf_counter()\n",
    "    xor_bytes = zstd.decompress(compressed_xor)\n",
    "    xor_arr = np.frombuffer(xor_bytes, dtype=np.uint8).reshape(uint8_shape)\n",
    "    t_decompress = perf_counter() - t0\n",
    "    print(f\"  Decompressed XOR bytes in {t_decompress:.7f}s, shape: {xor_arr.shape}\")\n",
    "\n",
    "    # Get base data (for biases/norms only)\n",
    "    if is_bias or is_norm:\n",
    "        if is_mha and is_bias:\n",
    "            base_data = (module.in_proj_bias if 'in_proj_bias' in param_name else module.out_proj_bias).data.cpu().numpy()\n",
    "        else:\n",
    "            base_data = (module.bias if is_bias else module.weight).data.cpu().numpy()\n",
    "        print(f\"  Base data shape: {base_data.shape}, dtype: {base_data.dtype}\")\n",
    "\n",
    "        # View as bytes, XOR, view back as float32, reshape\n",
    "        base_bytes = base_data.view(np.uint8).reshape(uint8_shape)\n",
    "        ft_bytes = np.bitwise_xor(base_bytes, xor_arr)\n",
    "        new_data = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(shape)\n",
    "        new_data = torch.from_numpy(new_data).to(dtype=torch.float32, device=device)\n",
    "        print(f\"  Finetuned data shape: {new_data.shape}, sample values: {new_data.flatten()[:5]}\")\n",
    "\n",
    "        track_timing(timing_data, tile_data, param_name, t_decompress)\n",
    "\n",
    "        if is_mha and is_bias:\n",
    "            print(f\"  DEBUG: is_mha={is_mha}, is_bias={is_bias} for {param_name} - updating in-place, NO HOOK\")\n",
    "            update_mha_bias_in_place(module, param_name, new_data, device)\n",
    "            return None  # Exit early, absolutely no hook\n",
    "        else:\n",
    "            # Non-MHA biases/norms: create hook\n",
    "            print(f\"  DEBUG: Creating forward hook for non-MHA {param_name}\")\n",
    "            def hook_fn(module, inputs, output):\n",
    "                call_count = getattr(hook_fn, 'call_count', 0)\n",
    "                hook_fn.call_count = call_count + 1\n",
    "                print(f\"  Applying hook for {param_name} (call #{call_count + 1}), input shape: {inputs[0].shape}\")\n",
    "                if is_norm:\n",
    "                    output = output * new_data\n",
    "                    print(f\"  Applied norm multiplication (output *= finetuned weight)\")\n",
    "                else:\n",
    "                    output_shape = output.shape if not is_mha else output[0].shape\n",
    "                    if isinstance(module, nn.LayerNorm):\n",
    "                        batch, seq, dim = output_shape\n",
    "                        bias_expanded = new_data.unsqueeze(0).unsqueeze(1).expand(batch, seq, -1)\n",
    "                        output = output + bias_expanded\n",
    "                        print(f\"  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = {bias_expanded.shape})\")\n",
    "                    else:\n",
    "                        bias_expanded = new_data.unsqueeze(0).expand(output_shape[0], -1)\n",
    "                        output = output + bias_expanded if not is_mha else (output[0] + bias_expanded, output[1])\n",
    "                        print(f\"  Applied bias addition (expanded to [batch, dim] = {bias_expanded.shape})\")\n",
    "                print(f\"  Output shape after hook: {output.shape if not is_mha else output[0].shape}\")\n",
    "                return output\n",
    "\n",
    "            print(f\"  Created and returning hook function for {param_name}\")\n",
    "            return hook_fn\n",
    "    else:\n",
    "        # Weights: skip for now\n",
    "        print(f\"  Skipping weight {param_name} (tiling in later steps)\")\n",
    "        track_timing(timing_data, tile_data, param_name, 0.0)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION: Visualize metrics # -----------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "def visualize_metrics(timing_data, size_data, tile_data):\n",
    "    if timing_data['param_names']:\n",
    "        timing_chart = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": timing_data['param_names'],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Decompress Time (s)\",\n",
    "                        \"data\": timing_data['t_partial_decompress'],\n",
    "                        \"backgroundColor\": \"rgba(75, 192, 192, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(75, 192, 192, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"XOR Patch Time (s)\",\n",
    "                        \"data\": timing_data['t_tile_xor_patch'],\n",
    "                        \"backgroundColor\": \"rgba(255, 99, 132, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(255, 99, 132, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Forward Tile Time (s)\",\n",
    "                        \"data\": timing_data['t_forward_tile'],\n",
    "                        \"backgroundColor\": \"rgba(54, 162, 235, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(54, 162, 235, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": \"Timing Breakdown per Parameter\"\n",
    "                    }\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"beginAtZero\": True,\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Time (seconds)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Parameter\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(\"\\nTiming Chart JSON:\")\n",
    "        print(json.dumps(timing_chart, indent=2))\n",
    "\n",
    "        size_chart = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": size_data['param_names'],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Number of Elements\",\n",
    "                        \"data\": size_data['numel'],\n",
    "                        \"backgroundColor\": \"rgba(153, 102, 255, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(153, 102, 255, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": \"Parameter Sizes (Number of Elements)\"\n",
    "                    }\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"beginAtZero\": True,\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Number of Elements\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Parameter\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(\"\\nSize Chart JSON:\")\n",
    "        print(json.dumps(size_chart, indent=2))\n",
    "\n",
    "        tile_chart = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": tile_data['param_names'],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Number of Tiles\",\n",
    "                        \"data\": tile_data['num_tiles'],\n",
    "                        \"backgroundColor\": \"rgba(255, 159, 64, 0.6)\",\n",
    "                        \"borderColor\": \"rgba(255, 159, 64, 1)\",\n",
    "                        \"borderWidth\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": \"Number of Tiles per Parameter\"\n",
    "                    }\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"beginAtZero\": True,\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Number of Tiles\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Parameter\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(\"\\nTile Chart JSON:\")\n",
    "        print(json.dumps(tile_chart, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1 - Starting Function ===\n",
      "Input x shape: torch.Size([4, 16, 64]), device: cpu, tile_rows: 64, tile_cols: 64, test_mode: False\n",
      "Package load time: 0.0000205s\n",
      "pkg keys: ['meta', 'deltas', 'zstd_level', 'stats']\n",
      "Model param names: ['ln1.weight', 'ln1.bias', 'self_attn.in_proj_weight', 'self_attn.in_proj_bias', 'self_attn.out_proj.weight', 'self_attn.out_proj.bias', 'ln2.weight', 'ln2.bias', 'ff.0.weight', 'ff.0.bias', 'ff.2.weight', 'ff.2.bias']\n",
      "Tiny layer params (numel): [64, 64, 12288, 192, 4096, 64, 64, 64, 8192, 128, 8192, 64]\n",
      "Total parameters: 33472\n",
      "Model param shapes:\n",
      "  ln1.weight: torch.Size([64])\n",
      "  ln1.bias: torch.Size([64])\n",
      "  self_attn.in_proj_weight: torch.Size([192, 64])\n",
      "  self_attn.in_proj_bias: torch.Size([192])\n",
      "  self_attn.out_proj.weight: torch.Size([64, 64])\n",
      "  self_attn.out_proj.bias: torch.Size([64])\n",
      "  ln2.weight: torch.Size([64])\n",
      "  ln2.bias: torch.Size([64])\n",
      "  ff.0.weight: torch.Size([128, 64])\n",
      "  ff.0.bias: torch.Size([128])\n",
      "  ff.2.weight: torch.Size([64, 128])\n",
      "  ff.2.bias: torch.Size([64])\n",
      "Delta keys (updates): {'ff.2.weight', 'ff.0.bias', 'ln1.weight', 'ln1.bias', 'ln2.bias', 'self_attn.out_proj.weight', 'self_attn.in_proj_bias', 'ff.2.bias', 'self_attn.out_proj.bias', 'ln2.weight', 'self_attn.in_proj_weight', 'ff.0.weight'}\n",
      "Param keys (model): {'ff.2.weight', 'ff.0.bias', 'ln1.weight', 'ln1.bias', 'ln2.bias', 'self_attn.out_proj.weight', 'self_attn.in_proj_bias', 'ff.2.bias', 'self_attn.out_proj.bias', 'ln2.weight', 'self_attn.in_proj_weight', 'ff.0.weight'}\n",
      "\n",
      "=== Step 2 - Module Collection ===\n",
      "Linear layers: ['self_attn.out_proj', 'ff.0', 'ff.2']\n",
      "Norm layers: ['ln1', 'ln2']\n",
      "MHA layers: ['self_attn']\n",
      "\n",
      "=== Step 3 - Initialize Visualization Data Structures ===\n",
      "Initialized tracking data structures:\n",
      "  Timing keys: ['param_names', 't_partial_decompress', 't_tile_xor_patch', 't_forward_tile']\n",
      "  Size keys: ['param_names', 'numel']\n",
      "  Tile keys: ['param_names', 'num_tiles']\n",
      "\n",
      "=== Registering hooks for linear layers ===\n",
      "  Processing linear layer 'self_attn.out_proj': weight='self_attn.out_proj.weight', bias='self_attn.out_proj.bias'\n",
      "-----------------self_attn.out_proj.weight Hook-----------------\n",
      "False False False\n",
      "  Shape: [64, 64], uint8_shape: [64, 256], numel: 4096, bytes: 16384\n",
      "  Decompressed XOR bytes in 0.0000301s, shape: (64, 256)\n",
      "  Skipping weight self_attn.out_proj.weight (tiling in later steps)\n",
      "-----------------self_attn.out_proj.bias Hook-----------------\n",
      "False True False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000039s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: Creating forward hook for non-MHA self_attn.out_proj.bias\n",
      "  Created and returning hook function for self_attn.out_proj.bias\n",
      "    Registered bias hook for 'self_attn.out_proj.bias'\n",
      "  Processing linear layer 'ff.0': weight='ff.0.weight', bias='ff.0.bias'\n",
      "-----------------ff.0.weight Hook-----------------\n",
      "False False False\n",
      "  Shape: [128, 64], uint8_shape: [128, 256], numel: 8192, bytes: 32768\n",
      "  Decompressed XOR bytes in 0.0000174s, shape: (128, 256)\n",
      "  Skipping weight ff.0.weight (tiling in later steps)\n",
      "-----------------ff.0.bias Hook-----------------\n",
      "False True False\n",
      "  Shape: [128], uint8_shape: [512], numel: 128, bytes: 512\n",
      "  Decompressed XOR bytes in 0.0000022s, shape: (512,)\n",
      "  Base data shape: (128,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([128]), sample values: tensor([ 0.0040, -0.0448, -0.0866,  0.0217,  0.1057])\n",
      "  DEBUG: Creating forward hook for non-MHA ff.0.bias\n",
      "  Created and returning hook function for ff.0.bias\n",
      "    Registered bias hook for 'ff.0.bias'\n",
      "  Processing linear layer 'ff.2': weight='ff.2.weight', bias='ff.2.bias'\n",
      "-----------------ff.2.weight Hook-----------------\n",
      "False False False\n",
      "  Shape: [64, 128], uint8_shape: [64, 512], numel: 8192, bytes: 32768\n",
      "  Decompressed XOR bytes in 0.0000150s, shape: (64, 512)\n",
      "  Skipping weight ff.2.weight (tiling in later steps)\n",
      "-----------------ff.2.bias Hook-----------------\n",
      "False True False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000017s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([-0.0282,  0.0206, -0.0500, -0.0040, -0.0768])\n",
      "  DEBUG: Creating forward hook for non-MHA ff.2.bias\n",
      "  Created and returning hook function for ff.2.bias\n",
      "    Registered bias hook for 'ff.2.bias'\n",
      "  Total linear hooks registered: 3\n",
      "\n",
      "=== Registering hooks for norm layers ===\n",
      "  Processing norm layer 'ln1': weight='ln1.weight', bias='ln1.bias'\n",
      "-----------------ln1.weight Hook-----------------\n",
      "False False True\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000039s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([1., 1., 1., 1., 1.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln1.weight\n",
      "  Created and returning hook function for ln1.weight\n",
      "-----------------ln1.bias Hook-----------------\n",
      "False True False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000033s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln1.bias\n",
      "  Created and returning hook function for ln1.bias\n",
      "    Registered weight hook for 'ln1.weight'\n",
      "    Registered bias hook for 'ln1.bias'\n",
      "  Processing norm layer 'ln2': weight='ln2.weight', bias='ln2.bias'\n",
      "-----------------ln2.weight Hook-----------------\n",
      "False False True\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000029s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([1., 1., 1., 1., 1.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln2.weight\n",
      "  Created and returning hook function for ln2.weight\n",
      "-----------------ln2.bias Hook-----------------\n",
      "False True False\n",
      "  Shape: [64], uint8_shape: [256], numel: 64, bytes: 256\n",
      "  Decompressed XOR bytes in 0.0000035s, shape: (256,)\n",
      "  Base data shape: (64,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([64]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: Creating forward hook for non-MHA ln2.bias\n",
      "  Created and returning hook function for ln2.bias\n",
      "    Registered weight hook for 'ln2.weight'\n",
      "    Registered bias hook for 'ln2.bias'\n",
      "  Processed 6 linear parameters, 4 norm parameters (total attempts: 10)\n",
      "  Actual hooks registered so far: 7 (placeholders return None)\n",
      "\n",
      "=== Registering hooks for MHA layers ===\n",
      "  Processing MHA layer 'self_attn':\n",
      "    Processing weight 'self_attn.in_proj_weight'\n",
      "-----------------self_attn.in_proj_weight Hook-----------------\n",
      "True False False\n",
      "  Shape: [192, 64], uint8_shape: [192, 256], numel: 12288, bytes: 49152\n",
      "  Decompressed XOR bytes in 0.0000156s, shape: (192, 256)\n",
      "  Skipping weight self_attn.in_proj_weight (tiling in later steps)\n",
      "    Processing weight 'self_attn.out_proj_weight'\n",
      "-----------------self_attn.out_proj_weight Hook-----------------\n",
      "True False False\n",
      "Skipping self_attn.out_proj_weight: no update in pkg['deltas']\n",
      "    Processing bias 'self_attn.in_proj_bias'\n",
      "-----------------self_attn.in_proj_bias Hook-----------------\n",
      "True True False\n",
      "  Shape: [192], uint8_shape: [768], numel: 192, bytes: 768\n",
      "  Decompressed XOR bytes in 0.0000023s, shape: (768,)\n",
      "  Base data shape: (192,), dtype: float32\n",
      "  Finetuned data shape: torch.Size([192]), sample values: tensor([0., 0., 0., 0., 0.])\n",
      "  DEBUG: is_mha=True, is_bias=True for self_attn.in_proj_bias - updating in-place, NO HOOK\n",
      "  Updated MHA bias parameter in-place for self_attn.in_proj_bias\n",
      "    Processing bias 'self_attn.out_proj_bias'\n",
      "-----------------self_attn.out_proj_bias Hook-----------------\n",
      "True True False\n",
      "Skipping self_attn.out_proj_bias: no update in pkg['deltas']\n",
      "  Processed 4 MHA parameters (total attempts: 14)\n",
      "  Actual hooks registered so far: 7 (placeholders return None)\n",
      "\n",
      "=== Running forward pass with hooks ===\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight, input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias, input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight (call #2), input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias (call #2), input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.weight (call #1), input shape: torch.Size([4, 16, 64])\n",
      "  Applied norm multiplication (output *= finetuned weight)\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for ln1.bias (call #1), input shape: torch.Size([4, 16, 64])\n",
      "  Applied LayerNorm bias addition (expanded to [batch, seq, dim] = torch.Size([4, 16, 64]))\n",
      "  Output shape after hook: torch.Size([4, 16, 64])\n",
      "  Applying hook for self_attn.in_proj_bias, input shape: torch.Size([4, 16, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (192) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRemoved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hook_handles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m hook handles\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m x_bench = torch.randn(\u001b[32m4\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m64\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m temp = \u001b[43me2e_forward_on_delta_no_full_reconstruct_TODO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_bench\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36me2e_forward_on_delta_no_full_reconstruct_TODO\u001b[39m\u001b[34m(base_layer, pkg_bytes, x, device, tile_rows, tile_cols, test_mode)\u001b[39m\n\u001b[32m    111\u001b[39m t0 = perf_counter()\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     output = \u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m t_total = perf_counter() - t0\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForward pass time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayung\\Desktop\\Github\\delta-serving\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayung\\Desktop\\Github\\delta-serving\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mTinyTransformerLayer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     38\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.ln1(x)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     a, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     x = x + a\n\u001b[32m     41\u001b[39m     h2 = \u001b[38;5;28mself\u001b[39m.ln2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayung\\Desktop\\Github\\delta-serving\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayung\\Desktop\\Github\\delta-serving\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1879\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1881\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1882\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayung\\Desktop\\Github\\delta-serving\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1840\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1838\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1839\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1843\u001b[39m     result = hook_result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mcreate_tiled_hook.<locals>.hook_fn\u001b[39m\u001b[34m(module, inputs, output)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     64\u001b[39m         bias_expanded = new_data.unsqueeze(\u001b[32m0\u001b[39m).expand(output_shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         output = output + bias_expanded \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mha \u001b[38;5;28;01melse\u001b[39;00m (\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_expanded\u001b[49m, output[\u001b[32m1\u001b[39m])\n\u001b[32m     66\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Applied bias addition (expanded to [batch, dim] = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias_expanded.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Output shape after hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39mis_mha\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39moutput[\u001b[32m0\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (64) must match the size of tensor b (192) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TODO: compute on decompressed XOR without global XOR\n",
    "# -----------------------------\n",
    "def e2e_forward_on_delta_no_full_reconstruct_TODO(base_layer: nn.Module, pkg_bytes: bytes, x: torch.Tensor,\n",
    "                                                  *, device: str = 'cpu', tile_rows: int = 64, tile_cols: int = 64,\n",
    "                                                  test_mode: bool = False):\n",
    "    print(f\"\\n=== Step 1 - Starting Function ===\")\n",
    "    print(f\"Input x shape: {x.shape}, device: {device}, tile_rows: {tile_rows}, tile_cols: {tile_cols}, test_mode: {test_mode}\")\n",
    "    t0 = perf_counter()\n",
    "    pkg = pickle.loads(pkg_bytes)\n",
    "    t_load = perf_counter() - t0\n",
    "    print(f\"Package load time: {t_load:.7f}s\")\n",
    "    print(f\"pkg keys: {list(pkg.keys())}\")\n",
    "    print(f\"Model param names: {[name for name, _ in base_layer.named_parameters()]}\")\n",
    "    print(f\"Tiny layer params (numel): {[p.numel() for p in base_layer.parameters()]}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in base_layer.parameters())}\")\n",
    "\n",
    "    print(f\"Model param shapes:\")\n",
    "    for name, p in base_layer.named_parameters():\n",
    "        print(f\"  {name}: {p.shape}\")\n",
    "\n",
    "    delta_keys = set(pkg['deltas'].keys())\n",
    "    param_keys = set(name for name, _ in base_layer.named_parameters())\n",
    "    print(f\"Delta keys (updates): {delta_keys}\")\n",
    "    print(f\"Param keys (model): {param_keys}\")\n",
    "    if not delta_keys.issubset(param_keys):\n",
    "        print(f\"Warning: pkg['deltas'] contains extra keys: {delta_keys - param_keys}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Step 2 - Module Collection ===\")\n",
    "    named_linears = [(name, module) for name, module in base_layer.named_modules() if isinstance(module, nn.Linear)]\n",
    "    named_norms = [(name, module) for name, module in base_layer.named_modules() if isinstance(module, nn.LayerNorm)]\n",
    "    named_mha = [(name, module) for name, module in base_layer.named_modules() if isinstance(module, nn.MultiheadAttention)]\n",
    "    print(f\"Linear layers: {[name for name, _ in named_linears]}\")\n",
    "    print(f\"Norm layers: {[name for name, _ in named_norms]}\")\n",
    "    print(f\"MHA layers: {[name for name, _ in named_mha]}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Step 3 - Initialize Visualization Data Structures ===\")\n",
    "    timing_data = {'param_names': [], 't_partial_decompress': [], 't_tile_xor_patch': [], 't_forward_tile': []}\n",
    "    size_data = {'param_names': [], 'numel': []}\n",
    "    tile_data = {'param_names': [], 'num_tiles': []}\n",
    "    print(f\"Initialized tracking data structures:\")\n",
    "    print(f\"  Timing keys: {list(timing_data.keys())}\")\n",
    "    print(f\"  Size keys: {list(size_data.keys())}\")\n",
    "    print(f\"  Tile keys: {list(tile_data.keys())}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n=== Registering hooks for linear layers ===\")\n",
    "    hook_handles = []\n",
    "    for name, module in named_linears:\n",
    "        weight_name = name + '.weight' if name else 'weight'\n",
    "        bias_name = name + '.bias' if name else 'bias'\n",
    "        print(f\"  Processing linear layer '{name}': weight='{weight_name}', bias='{bias_name}'\")\n",
    "        weight_hook = create_tiled_hook(weight_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data)\n",
    "        bias_hook = create_tiled_hook(bias_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=True)\n",
    "        if weight_hook:\n",
    "            hook_handles.append(module.register_forward_hook(weight_hook))\n",
    "            print(f\"    Registered weight hook for '{weight_name}'\")\n",
    "        if bias_hook:\n",
    "            hook_handles.append(module.register_forward_hook(bias_hook))\n",
    "            print(f\"    Registered bias hook for '{bias_name}'\")\n",
    "    print(f\"  Total linear hooks registered: {len(hook_handles)}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n=== Registering hooks for norm layers ===\")\n",
    "    linear_attempts = len(named_linears) * 2  # weight + bias per linear\n",
    "    norm_attempts = 0\n",
    "    for name, module in named_norms:\n",
    "        w_name = name + '.weight' if name else 'weight'\n",
    "        b_name = name + '.bias' if name else 'bias'\n",
    "        print(f\"  Processing norm layer '{name}': weight='{w_name}', bias='{b_name}'\")\n",
    "        w_hook = create_tiled_hook(w_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_norm=True)\n",
    "        b_hook = create_tiled_hook(b_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=True)\n",
    "        if w_hook:\n",
    "            hook_handles.append(module.register_forward_hook(w_hook))\n",
    "            print(f\"    Registered weight hook for '{w_name}'\")\n",
    "        if b_hook:\n",
    "            hook_handles.append(module.register_forward_hook(b_hook))\n",
    "            print(f\"    Registered bias hook for '{b_name}'\")\n",
    "        norm_attempts += 2  # weight + bias attempt\n",
    "    print(f\"  Processed {linear_attempts} linear parameters, {norm_attempts} norm parameters (total attempts: {linear_attempts + norm_attempts})\")\n",
    "    print(f\"  Actual hooks registered so far: {len(hook_handles)} (placeholders return None)\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Registering hooks for MHA layers ===\")\n",
    "    mha_attempts = 0\n",
    "    for name, module in named_mha:\n",
    "        print(f\"  Processing MHA layer '{name}':\")\n",
    "        # Weights\n",
    "        for w_name in ['in_proj_weight', 'out_proj_weight']:\n",
    "            full_name = name + '.' + w_name if name else w_name\n",
    "            print(f\"    Processing weight '{full_name}'\")\n",
    "            hook = create_tiled_hook(full_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_mha=True)\n",
    "            if hook:\n",
    "                hook_handles.append(module.register_forward_hook(hook))\n",
    "                print(f\"      Registered weight hook for '{full_name}'\")\n",
    "        # Biases\n",
    "        for b_name in ['in_proj_bias', 'out_proj_bias']:\n",
    "            full_name = name + '.' + b_name if name else b_name\n",
    "            print(f\"    Processing bias '{full_name}'\")\n",
    "            hook = create_tiled_hook(full_name, module, pkg, device, tile_rows, tile_cols, timing_data, size_data, tile_data, is_bias=True, is_mha=True)\n",
    "            if hook:\n",
    "                hook_handles.append(module.register_forward_hook(hook))\n",
    "                print(f\"      Registered bias hook for '{full_name}'\")\n",
    "        mha_attempts += 4  # 2 weights + 2 biases per MHA\n",
    "    print(f\"  Processed {mha_attempts} MHA parameters (total attempts: {linear_attempts + norm_attempts + mha_attempts})\")\n",
    "    print(f\"  Actual hooks registered so far: {len(hook_handles)} (placeholders return None)\")\n",
    "\n",
    "    print(f\"\\n=== Running forward pass with hooks ===\")\n",
    "    t0 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        output = base_layer(x.to(device))\n",
    "    t_total = perf_counter() - t0\n",
    "    print(f\"Forward pass time: {t_total:.7f}s\")\n",
    "    print(f\"Total time: {t_total + t_load:.7f}s (load: {t_load:.7f}s, forward: {t_total:.7f}s)\")\n",
    "    print(f\"Final output shape: {output.shape if not isinstance(output, tuple) else output[0].shape}\")\n",
    "\n",
    "    print(f\"\\n=== Cleaning up hooks ===\")\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    print(f\"Removed {len(hook_handles)} hook handles\")\n",
    "\n",
    "\n",
    "x_bench = torch.randn(4, 16, 64)\n",
    "temp = e2e_forward_on_delta_no_full_reconstruct_TODO(base_layer, pkg_bytes, x_bench, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
