{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR + Zstd Compression Demo on a Tiny Transformer Layer\n",
    "\n",
    "**Outline:** Show how to (1) create a tiny Transformer layer, (2) simulate finetuning by perturbing a small subset of parameters, (3) compress the finetuned weights as XOR against the base, then Zstd-compress the XOR result, and (4) decompress to **exactly** recover the finetuned layer and run a forward pass.\n",
    "\n",
    "**Why this demo:** It mirrors our current workflow and marks the exact place we want to optimize next—*computing on XOR-compressed data directly (without full decompression)*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e2929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# If needed, install deps:\n",
    "# !pip install torch zstandard\n",
    "\n",
    "import math, io, pickle, time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn|\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    import zstandard as zstd\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"Please install `zstandard` (pip install zstandard) before running this cell.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny Transformer layer\n",
    "# -----------------------------\n",
    "class TinyTransformerLayer(nn.Module):\n",
    "    \"\"\"Small block with MHA + FFN, sufficient for demonstration.\"\"\"\n",
    "    def __init__(self, d_model=64, nhead=4, dim_feedforward=128, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.self_attn(h, h, h, attn_mask=attn_mask)\n",
    "        x = x + a\n",
    "        h2 = self.ln2(x)\n",
    "        y = self.ff(h2)\n",
    "        return x + y\n",
    "\n",
    "base_layer = TinyTransformerLayer().eval()\n",
    "print(\"Tiny layer params:\", sum(p.numel() for p in base_layer.parameters()))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simulate finetuning (sparse updates)\n",
    "# -----------------------------\n",
    "from copy import deepcopy\n",
    "\n",
    "def perturb_subset_(module: nn.Module, frac: float = 0.03, magnitude: float = 1e-2, seed: int = 0):\n",
    "    \"\"\"In-place: perturb `frac` of elements in each float parameter.\n",
    "    This simulates sparse finetuning, making XOR(base, finetuned) mostly zeros.\n",
    "    \"\"\"\n",
    "    g = torch.Generator(device='cpu').manual_seed(seed)\n",
    "    with torch.no_grad():\n",
    "        for name, p in module.named_parameters():\n",
    "            if not p.dtype.is_floating_point:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            k = max(1, int(numel * frac))\n",
    "            idx = torch.randperm(numel, generator=g, device=p.device)[:k]\n",
    "            flat = p.view(-1)\n",
    "            noise = magnitude * torch.randn(k, generator=g, device=p.device, dtype=flat.dtype)\n",
    "            flat[idx] += noise\n",
    "\n",
    "finetuned_layer = deepcopy(base_layer)\n",
    "perturb_subset_(finetuned_layer, frac=0.03, magnitude=1e-2, seed=123)\n",
    "\n",
    "# Count changed elements (exact because we add noise)\n",
    "changed, total = 0, 0\n",
    "for (n1, p_base), (n2, p_ft) in zip(base_layer.named_parameters(), finetuned_layer.named_parameters()):\n",
    "    assert n1 == n2\n",
    "    if p_base.dtype.is_floating_point:\n",
    "        diff = (p_base != p_ft).sum().item()\n",
    "        changed += diff\n",
    "        total += p_base.numel()\n",
    "print(f\"Changed elements: {changed} / {total} (~{changed/total*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR + Zstd compression helpers\n",
    "# -----------------------------\n",
    "def tensor_to_uint8_view(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Return a NumPy uint8 *view* over tensor bytes (CPU).\"\"\"\n",
    "    a = t.detach().cpu().numpy()\n",
    "    return a.view(np.uint8)\n",
    "\n",
    "def xor_uint8(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    assert a.dtype == np.uint8 and b.dtype == np.uint8 and a.size == b.size\n",
    "    return np.bitwise_xor(a, b)\n",
    "\n",
    "def compress_state_dict_with_xor_zstd(base_sd: dict, finetuned_sd: dict, level: int = 10):\n",
    "    \"\"\"Pack XOR(Zstd) deltas.\n",
    "    Returns a dict with:\n",
    "      - meta[name]: shape/dtype/numel/param_bytes/uint8_shape\n",
    "      - deltas[name]: Zstd-compressed XOR bytes\n",
    "      - stats: aggregate sizes\n",
    "    \"\"\"\n",
    "    compressor = zstd.ZstdCompressor(level=level)\n",
    "    meta, deltas = {}, {}\n",
    "    stats = {'total_params_bytes': 0, 'total_delta_bytes': 0, 'total_compressed_bytes': 0, 'per_param': {}}\n",
    "\n",
    "    for name, base_t in base_sd.items():\n",
    "        ft_t = finetuned_sd[name]\n",
    "        base_bytes = tensor_to_uint8_view(base_t)\n",
    "        ft_bytes = tensor_to_uint8_view(ft_t)\n",
    "        assert base_bytes.size == ft_bytes.size, name\n",
    "\n",
    "        delta_u8 = xor_uint8(base_bytes, ft_bytes)\n",
    "        raw_delta = delta_u8.tobytes()\n",
    "        comp = compressor.compress(raw_delta)\n",
    "\n",
    "        meta[name] = {\n",
    "            'shape': list(ft_t.shape),\n",
    "            'dtype_str': str(ft_t.detach().cpu().numpy().dtype.str),  # e.g. '<f4'\n",
    "            'numel': ft_t.numel(),\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'uint8_shape': list(base_bytes.shape),\n",
    "        }\n",
    "        deltas[name] = comp\n",
    "\n",
    "        stats['total_params_bytes'] += ft_bytes.size\n",
    "        stats['total_delta_bytes'] += len(raw_delta)\n",
    "        stats['total_compressed_bytes'] += len(comp)\n",
    "        stats['per_param'][name] = {\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'delta_bytes': len(raw_delta),\n",
    "            'compressed_bytes': len(comp),\n",
    "            'compressed_ratio_vs_param': len(comp) / max(1, ft_bytes.size),\n",
    "        }\n",
    "\n",
    "    return {'meta': meta, 'deltas': deltas, 'zstd_level': level, 'stats': stats}\n",
    "\n",
    "def reconstruct_finetuned_state_dict_with_timing(base_sd: dict, pkg: dict):\n",
    "    \"\"\"Decompress and XOR-apply with timing breakdown.\n",
    "    Returns: (rec_sd, timing_dict) with keys: t_decompress, t_xor_apply\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    rec = {}\n",
    "    t_decomp = 0.0\n",
    "    t_xor = 0.0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        # Decompress\n",
    "        t0 = perf_counter()\n",
    "        raw_delta = dctx.decompress(comp)\n",
    "        t_decomp += perf_counter() - t0\n",
    "        delta_u8 = np.frombuffer(raw_delta, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "\n",
    "        # XOR apply\n",
    "        base_bytes = tensor_to_uint8_view(base_sd[name])\n",
    "        t1 = perf_counter()\n",
    "        ft_bytes = np.bitwise_xor(base_bytes, delta_u8)  # shape-aligned XOR\n",
    "        t_xor += perf_counter() - t1\n",
    "\n",
    "        # Reinterpret as original dtype/shape\n",
    "        arr = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(meta['shape']).copy()\n",
    "        rec[name] = torch.from_numpy(arr)\n",
    "    return rec, {'t_decompress': t_decomp, 't_xor_apply': t_xor}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build base/finetuned SDs and compress\n",
    "# -----------------------------\n",
    "base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "ft_sd   = {k: v.detach().cpu().contiguous() for k, v in finetuned_layer.state_dict().items()}\n",
    "\n",
    "pkg = compress_state_dict_with_xor_zstd(base_sd, ft_sd, level=10)\n",
    "print(\"=== Compression Stats ===\")\n",
    "print(\"Total param bytes  :\", pkg['stats']['total_params_bytes'])\n",
    "print(\"Total delta bytes  :\", pkg['stats']['total_delta_bytes'])\n",
    "print(\"Total compressed   :\", pkg['stats']['total_compressed_bytes'])\n",
    "print(\"Global ratio       :\", pkg['stats']['total_compressed_bytes']/pkg['stats']['total_params_bytes'])\n",
    "for name, st in list(pkg['stats']['per_param'].items())[:5]:\n",
    "    print(f\"- {name:40s} | param={st['param_bytes']} | comp={st['compressed_bytes']} | ratio={st['compressed_ratio_vs_param']:.4f}\")\n",
    "\n",
    "# Serialize to in-memory bytes (so we can measure \"load\" as deserialization;\n",
    "# replace with actual disk I/O in your system if you want true file load timings).\n",
    "pkg_bytes = pickle.dumps(pkg)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end function with timers (load → reconstruct → load_state_dict → forward)\n",
    "# -----------------------------\n",
    "def e2e_forward_from_pkg_bytes(base_layer: nn.Module, pkg_bytes: bytes, x: torch.Tensor, device: str = 'cpu'):\n",
    "    \"\"\"End-to-end: load (deserialize) → decompress → XOR-apply → load_state_dict → forward.\n",
    "    Returns (timings_dict, checksum).\n",
    "    NOTE: 't_load_pkg' here measures pickle.loads (in-memory). Replace with disk I/O for on-disk timings.\n",
    "    \"\"\"\n",
    "    # 0) Prepare inputs\n",
    "    x = x.to(device)\n",
    "    base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "\n",
    "    # 1) Load package (deserialize)\n",
    "    t0 = perf_counter()\n",
    "    pkg = pickle.loads(pkg_bytes)\n",
    "    t_load = perf_counter() - t0\n",
    "\n",
    "    # 2) Decompress + XOR apply (split timings)\n",
    "    rec_sd, t_parts = reconstruct_finetuned_state_dict_with_timing(base_sd, pkg)\n",
    "\n",
    "    # 3) Materialize a new layer and copy weights\n",
    "    t2 = perf_counter()\n",
    "    rec_layer = TinyTransformerLayer().to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for k, p in rec_layer.state_dict().items():\n",
    "            p.copy_(rec_sd[k].to(device))\n",
    "    t_state = perf_counter() - t2\n",
    "\n",
    "    # 4) Forward once (demo)\n",
    "    t3 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        y = rec_layer(x)\n",
    "    t_fwd = perf_counter() - t3\n",
    "\n",
    "    timings = {\n",
    "        't_load_pkg': t_load,\n",
    "        't_decompress': t_parts['t_decompress'],\n",
    "        't_xor_apply': t_parts['t_xor_apply'],\n",
    "        't_state_load': t_state,\n",
    "        't_forward': t_fwd,\n",
    "        't_end_to_end': t_load + t_parts['t_decompress'] + t_parts['t_xor_apply'] + t_state + t_fwd,\n",
    "    }\n",
    "    checksum = float(y.abs().sum().detach().cpu())  # simple checksum to ensure consistent output\n",
    "    return timings, checksum\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Micro-benchmark: mean ± std per stage\n",
    "# -----------------------------\n",
    "def benchmark_e2e_breakdown(base_layer, pkg_bytes: bytes, x: torch.Tensor, device: str = 'cpu', repeats: int = 7):\n",
    "    \"\"\"Repeat the E2E pipeline `repeats` times and report mean/std for each stage.\"\"\"\n",
    "    keys = ['t_load_pkg','t_decompress','t_xor_apply','t_state_load','t_forward','t_end_to_end']\n",
    "    buf = {k: [] for k in keys}\n",
    "    checks = []\n",
    "    for _ in range(repeats):\n",
    "        timings, checksum = e2e_forward_from_pkg_bytes(base_layer, pkg_bytes, x, device=device)\n",
    "        for k in keys:\n",
    "            buf[k].append(timings[k])\n",
    "        checks.append(checksum)\n",
    "    stats = {k: {'mean': float(np.mean(v)), 'std': float(np.std(v))} for k, v in buf.items()}\n",
    "    return stats, checks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# What-if speed windows\n",
    "# -----------------------------\n",
    "def print_what_if_bounds(stats):\n",
    "    \"\"\"Compute what-if bounds from measured means:\n",
    "       - Bound A: skip global XOR (still decompress) → compute on decompressed XOR bytes\n",
    "       - Bound B: skip XOR + decompress (theoretical Zstd-domain compute)\n",
    "    \"\"\"\n",
    "    t_load = stats['t_load_pkg']['mean']\n",
    "    t_decomp = stats['t_decompress']['mean']\n",
    "    t_xor = stats['t_xor_apply']['mean']\n",
    "    t_state = stats['t_state_load']['mean']\n",
    "    t_fwd = stats['t_forward']['mean']\n",
    "    t_e2e = stats['t_end_to_end']['mean']\n",
    "\n",
    "    t_boundA = t_load + t_decomp + t_state + t_fwd       # skip XOR\n",
    "    t_boundB = t_load + t_state + t_fwd                  # skip XOR + decompress (theoretical)\n",
    "\n",
    "    def fmt_speedup(old, new):\n",
    "        return f\"{old/new:.2f}× faster (↓{(1 - new/old)*100:.1f}%)\" if new > 0 else \"∞\"\n",
    "\n",
    "    print(\"=== What-if Speed Windows (vs current E2E mean) ===\")\n",
    "    print(f\"Baseline E2E: {t_e2e:.6f}s\")\n",
    "    print(f\"Skip XOR (keep decompress): {t_boundA:.6f}s  → {fmt_speedup(t_e2e, t_boundA)}\")\n",
    "    print(f\"Skip XOR+Decompress (Zstd-domain compute): {t_boundB:.6f}s  → {fmt_speedup(t_e2e, t_boundB)}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- (Baseline → Bound A) is the best-case improvement if we avoid the global XOR-apply step by\")\n",
    "    print(\"  computing directly on decompressed XOR bytes (tile JIT patching, patch-based kernels, etc.).\")\n",
    "    print(\"- (Baseline → Bound B) is a theoretical ceiling if we could avoid both decompression and XOR\")\n",
    "    print(\"  (i.e., compute in Zstd domain); not realistic today, but shows maximum headroom.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Per-parameter hotspots (identify where XOR time concentrates)\n",
    "# -----------------------------\n",
    "def per_param_decompress_xor_timing(base_sd: dict, pkg: dict, topk: int = 5):\n",
    "    \"\"\"Per-parameter timing for decompress and XOR-apply to identify hotspots.\"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    records = []\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        t0 = perf_counter(); raw = dctx.decompress(comp); t_de = perf_counter()-t0\n",
    "        delta = np.frombuffer(raw, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "        t1 = perf_counter(); _ = np.bitwise_xor(tensor_to_uint8_view(base_sd[name]), delta); t_x = perf_counter()-t1\n",
    "        records.append((name, meta['param_bytes'], t_de, t_x))\n",
    "    # sort by XOR time (or total)\n",
    "    records.sort(key=lambda r: r[3], reverse=True)\n",
    "    print(f\"=== Top {topk} params by XOR-apply time ===\")\n",
    "    for name, sz, tde, tx in records[:topk]:\n",
    "        print(f\"{name:40s} | bytes={sz:8d} | decompress={tde:.6f}s | xor={tx:.6f}s\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR sparsity & naive sparse storage estimate\n",
    "# -----------------------------\n",
    "def estimate_xor_sparsity_and_storage(pkg: dict):\n",
    "    \"\"\"Measure XOR sparsity (non-zero bytes) and estimate a simple sparse storage cost.\n",
    "    Model: store (index:uint32 + value:uint8) per non-zero byte.\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    total_bytes = 0\n",
    "    total_nz = 0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        raw = dctx.decompress(pkg['deltas'][name])\n",
    "        arr = np.frombuffer(raw, dtype=np.uint8)\n",
    "        nz = int((arr != 0).sum())\n",
    "        total_bytes += arr.size\n",
    "        total_nz += nz\n",
    "    density = total_nz / max(1, total_bytes)\n",
    "    dense_bytes = total_bytes  # 1 byte per entry\n",
    "    # Sparse: 4 bytes for index + 1 byte value (very simple model)\n",
    "    sparse_bytes = total_nz * (4 + 1)\n",
    "    return {\n",
    "        'xor_total_bytes': total_bytes,\n",
    "        'xor_nonzero_bytes': total_nz,\n",
    "        'nonzero_density': density,\n",
    "        'dense_storage_bytes': dense_bytes,\n",
    "        'simple_sparse_storage_bytes': sparse_bytes,\n",
    "        'sparse_over_dense_ratio': sparse_bytes / max(1, dense_bytes),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run benchmarks\n",
    "# -----------------------------\n",
    "# End-to-end breakdown\n",
    "x_bench = torch.randn(4, 16, 64)\n",
    "stats, checks = benchmark_e2e_breakdown(base_layer, pkg_bytes, x_bench, device=device, repeats=7)\n",
    "print(\"=== Component-wise Timing (seconds): mean ± std over 7 runs ===\")\n",
    "for k, s in stats.items():\n",
    "    print(f\"{k:>14s}: {s['mean']:.6f} ± {s['std']:.6f}\")\n",
    "print(\"Sanity checksums (first 3):\", checks[:3])\n",
    "\n",
    "# What-if bounds\n",
    "print_what_if_bounds(stats)\n",
    "\n",
    "# Hotspots\n",
    "_ = per_param_decompress_xor_timing(base_sd, pkg, topk=5)\n",
    "\n",
    "# XOR sparsity summary\n",
    "sparsity = estimate_xor_sparsity_and_storage(pkg)\n",
    "print(\"\\n=== XOR Sparsity Estimate ===\")\n",
    "for k, v in sparsity.items():\n",
    "    print(f\"{k:28s}: {v}\")\n",
    "print(\"\\nNOTE: If non-zero density is low, consider block-sparse indexing (e.g., 256B/4KB blocks), \"\n",
    "      \"RLE on zero-runs, or bitplane packing so **many XOR adapters can co-reside on a single GPU.**\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a41ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TODO: compute on decompressed XOR without global XOR\n",
    "# -----------------------------\n",
    "def e2e_forward_on_delta_no_full_reconstruct_TODO(base_layer: nn.Module, pkg_bytes: bytes, x: torch.Tensor,\n",
    "                                                  *, device: str = 'cpu', tile_rows: int = 64, tile_cols: int = 64):\n",
    "    \"\"\"TODO (design & prototype): End-to-end pipeline that computes without full reconstruction.\n",
    "\n",
    "    Goal for a first prototype:\n",
    "      1) Deserialize pkg.\n",
    "      2) Convert a single Linear layer's weight (or a toy GEMM) into a tiled compute:\n",
    "         - For each tile, decode only the XOR bytes needed for that tile.\n",
    "         - XOR just-in-time into a small working buffer, run GEMM on that tile.\n",
    "      3) Measure timings: t_partial_decompress, t_tile_xor_patch, t_forward_tile, total.\n",
    "\n",
    "    NOTE: This stub does not implement the actual tiled kernel—it's where the optimization work begins.\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    pkg = pickle.loads(pkg_bytes)\n",
    "    print(pkg)\n",
    "    t_load = perf_counter() - t0\n",
    "\n",
    "    print(base_layer.size())\n",
    "\n",
    "    # Pseudocode structure (replace with a working tiled GEMM path):\n",
    "    raise NotImplementedError(\n",
    "        \"Prototype here: map layer weights to tiles, for each tile decompress-needed bytes \"\n",
    "        \"→ XOR into a small buffer → microkernel(GEMM) on that buffer. \"\n",
    "        \"Add timers: t_partial_decompress, t_tile_xor_patch, t_forward_tile.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Testing a TODO prototype\n",
    "\n",
    "temp = e2e_forward_on_delta_no_full_reconstruct_TODO(base_layer, pkg_bytes, x_bench, device=device)\n",
    "print(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf31947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# If needed, install deps:\n",
    "# !pip install torch zstandard\n",
    "\n",
    "import math, io, pickle, time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    import zstandard as zstd\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"Please install `zstandard` (pip install zstandard) before running this cell.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny Transformer layer\n",
    "# -----------------------------\n",
    "class TinyTransformerLayer(nn.Module):\n",
    "    \"\"\"Small block with MHA + FFN, sufficient for demonstration.\"\"\"\n",
    "    def __init__(self, d_model=64, nhead=4, dim_feedforward=128, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.self_attn(h, h, h, attn_mask=attn_mask)\n",
    "        x = x + a\n",
    "        h2 = self.ln2(x)\n",
    "        y = self.ff(h2)\n",
    "        return x + y\n",
    "\n",
    "base_layer = TinyTransformerLayer().eval()\n",
    "print(\"Tiny layer params:\", sum(p.numel() for p in base_layer.parameters()))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simulate finetuning (sparse updates)\n",
    "# -----------------------------\n",
    "from copy import deepcopy\n",
    "\n",
    "def perturb_subset_(module: nn.Module, frac: float = 0.03, magnitude: float = 1e-2, seed: int = 0):\n",
    "    \"\"\"In-place: perturb `frac` of elements in each float parameter.\n",
    "    This simulates sparse finetuning, making XOR(base, finetuned) mostly zeros.\n",
    "    \"\"\"\n",
    "    g = torch.Generator(device='cpu').manual_seed(seed)\n",
    "    with torch.no_grad():\n",
    "        for name, p in module.named_parameters():\n",
    "            if not p.dtype.is_floating_point:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            k = max(1, int(numel * frac))\n",
    "            idx = torch.randperm(numel, generator=g, device=p.device)[:k]\n",
    "            flat = p.view(-1)\n",
    "            noise = magnitude * torch.randn(k, generator=g, device=p.device, dtype=flat.dtype)\n",
    "            flat[idx] += noise\n",
    "\n",
    "# Two finetuned variants to simulate two tenants\n",
    "ft1 = deepcopy(base_layer); perturb_subset_(ft1, frac=0.03, magnitude=1e-2, seed=123)\n",
    "ft2 = deepcopy(base_layer); perturb_subset_(ft2, frac=0.05, magnitude=1e-2, seed=456)\n",
    "\n",
    "# Count changed elements (vs base) for the first one\n",
    "changed, total = 0, 0\n",
    "for (n1, p_base), (n2, p_ft) in zip(base_layer.named_parameters(), ft1.named_parameters()):\n",
    "    assert n1 == n2\n",
    "    if p_base.dtype.is_floating_point:\n",
    "        diff = (p_base != p_ft).sum().item()\n",
    "        changed += diff\n",
    "        total += p_base.numel()\n",
    "print(f\"[ft1] Changed elements: {changed} / {total} (~{changed/total*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# XOR + Zstd compression helpers (pack entire state_dict)\n",
    "# -----------------------------\n",
    "def tensor_to_uint8_view(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Return a NumPy uint8 *view* over tensor bytes (CPU).\"\"\"\n",
    "    a = t.detach().cpu().numpy()\n",
    "    return a.view(np.uint8)\n",
    "\n",
    "def xor_uint8(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    assert a.dtype == np.uint8 and b.dtype == np.uint8 and a.size == b.size\n",
    "    return np.bitwise_xor(a, b)\n",
    "\n",
    "def compress_state_dict_with_xor_zstd(base_sd: dict, finetuned_sd: dict, level: int = 10):\n",
    "    \"\"\"Pack XOR(Zstd) deltas.\n",
    "    Returns a dict with:\n",
    "      - meta[name]: shape/dtype/numel/param_bytes/uint8_shape\n",
    "      - deltas[name]: Zstd-compressed XOR bytes\n",
    "      - stats: aggregate sizes\n",
    "    \"\"\"\n",
    "    compressor = zstd.ZstdCompressor(level=level)\n",
    "    meta, deltas = {}, {}\n",
    "    stats = {'total_params_bytes': 0, 'total_delta_bytes': 0, 'total_compressed_bytes': 0, 'per_param': {}}\n",
    "\n",
    "    for name, base_t in base_sd.items():\n",
    "        ft_t = finetuned_sd[name]\n",
    "        base_bytes = tensor_to_uint8_view(base_t)\n",
    "        ft_bytes = tensor_to_uint8_view(ft_t)\n",
    "        assert base_bytes.size == ft_bytes.size, name\n",
    "\n",
    "        delta_u8 = xor_uint8(base_bytes, ft_bytes)\n",
    "        raw_delta = delta_u8.tobytes()\n",
    "        comp = compressor.compress(raw_delta)\n",
    "\n",
    "        meta[name] = {\n",
    "            'shape': list(ft_t.shape),\n",
    "            'dtype_str': str(ft_t.detach().cpu().numpy().dtype.str),  # e.g. '<f4'\n",
    "            'numel': ft_t.numel(),\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'uint8_shape': list(base_bytes.shape),\n",
    "        }\n",
    "        deltas[name] = comp\n",
    "\n",
    "        stats['total_params_bytes'] += ft_bytes.size\n",
    "        stats['total_delta_bytes'] += len(raw_delta)\n",
    "        stats['total_compressed_bytes'] += len(comp)\n",
    "        stats['per_param'][name] = {\n",
    "            'param_bytes': ft_bytes.size,\n",
    "            'delta_bytes': len(raw_delta),\n",
    "            'compressed_bytes': len(comp),\n",
    "            'compressed_ratio_vs_param': len(comp) / max(1, ft_bytes.size),\n",
    "        }\n",
    "\n",
    "    return {'meta': meta, 'deltas': deltas, 'zstd_level': level, 'stats': stats}\n",
    "\n",
    "def reconstruct_finetuned_state_dict_with_timing(base_sd: dict, pkg: dict):\n",
    "    \"\"\"Decompress and XOR-apply with timing breakdown.\n",
    "    Returns: (rec_sd, timing_dict) with keys: t_decompress, t_xor_apply\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    rec = {}\n",
    "    t_decomp = 0.0\n",
    "    t_xor = 0.0\n",
    "    for name, meta in pkg['meta'].items():\n",
    "        comp = pkg['deltas'][name]\n",
    "        # Decompress\n",
    "        t0 = perf_counter()\n",
    "        raw_delta = dctx.decompress(comp)\n",
    "        t_decomp += perf_counter() - t0\n",
    "        delta_u8 = np.frombuffer(raw_delta, dtype=np.uint8).reshape(meta['uint8_shape'])\n",
    "\n",
    "        # XOR apply\n",
    "        base_bytes = tensor_to_uint8_view(base_sd[name])\n",
    "        t1 = perf_counter()\n",
    "        ft_bytes = np.bitwise_xor(base_bytes, delta_u8)  # shape-aligned XOR\n",
    "        t_xor += perf_counter() - t1\n",
    "\n",
    "        # Reinterpret as original dtype/shape\n",
    "        arr = ft_bytes.view(np.dtype(meta['dtype_str'])).reshape(meta['shape']).copy()\n",
    "        rec[name] = torch.from_numpy(arr)\n",
    "    return rec, {'t_decompress': t_decomp, 't_xor_apply': t_xor}\n",
    "\n",
    "\n",
    "# Build packages for two tenants\n",
    "base_sd = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "pkg1 = compress_state_dict_with_xor_zstd(base_sd, {k: v.detach().cpu().contiguous() for k, v in ft1.state_dict().items()}, level=10)\n",
    "pkg2 = compress_state_dict_with_xor_zstd(base_sd, {k: v.detach().cpu().contiguous() for k, v in ft2.state_dict().items()}, level=10)\n",
    "pkg_list = [pkg1, pkg2]\n",
    "print(\"Tenants:\", len(pkg_list))\n",
    "print(\"pkg1 global ratio:\", pkg1['stats']['total_compressed_bytes']/pkg1['stats']['total_params_bytes'])\n",
    "print(\"pkg2 global ratio:\", pkg2['stats']['total_compressed_bytes']/pkg2['stats']['total_params_bytes'])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Standard E2E baseline (deserialize → decompress → XOR → load_state → forward)\n",
    "# -----------------------------\n",
    "def e2e_forward_from_pkg(base_layer: nn.Module, pkg: dict, x: torch.Tensor, device: str = 'cpu'):\n",
    "    \"\"\"End-to-end: decompress → XOR-apply → load_state_dict → forward (pkg already in memory).\"\"\"\n",
    "    # Inputs\n",
    "    x = x.to(device)\n",
    "    base_sd_local = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "\n",
    "    # Decompress + XOR apply\n",
    "    rec_sd, t_parts = reconstruct_finetuned_state_dict_with_timing(base_sd_local, pkg)\n",
    "\n",
    "    # Materialize a new layer and copy weights\n",
    "    t2 = perf_counter()\n",
    "    rec_layer = TinyTransformerLayer().to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for k, p in rec_layer.state_dict().items():\n",
    "            p.copy_(rec_sd[k].to(device))\n",
    "    t_state = perf_counter() - t2\n",
    "\n",
    "    # Forward once\n",
    "    t3 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        y = rec_layer(x)\n",
    "    t_fwd = perf_counter() - t3\n",
    "\n",
    "    timings = {\n",
    "        't_decompress': t_parts['t_decompress'],\n",
    "        't_xor_apply': t_parts['t_xor_apply'],\n",
    "        't_state_load': t_state,\n",
    "        't_forward': t_fwd,\n",
    "        't_end_to_end': t_parts['t_decompress'] + t_parts['t_xor_apply'] + t_state + t_fwd,\n",
    "    }\n",
    "    checksum = float(y.abs().sum().detach().cpu())\n",
    "    return timings, checksum\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ====== NEW: Multi-tenant serving with base-compute reuse (LoRA-style idea) ======\n",
    "# We only patch ff.0.weight for demonstration: reuse base forward up to h2,\n",
    "# compute y_base_ff1 once, add tenant-specific y_delta = ΔW @ x, then finish the layer.\n",
    "# -----------------------------\n",
    "\n",
    "def sparse_delta_from_pkg_for_linear(base_sd: dict, pkg: dict, param_name: str):\n",
    "    \"\"\"Extract a sparse ΔW for a Linear weight from XOR(Zstd) pkg by only decoding changed elements.\n",
    "\n",
    "    Returns:\n",
    "      rows: np.ndarray[int64], cols: np.ndarray[int64], delta_vals: np.ndarray[float32], (out_features, in_features)\n",
    "      and timing dict: {'t_param_decompress', 't_sparse_extract'}\n",
    "    \"\"\"\n",
    "    assert param_name in pkg['meta'], f\"{param_name} not in pkg\"\n",
    "    meta = pkg['meta'][param_name]\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "\n",
    "    # Decompress only this parameter's XOR bytes\n",
    "    t0 = perf_counter()\n",
    "    raw = dctx.decompress(pkg['deltas'][param_name])\n",
    "    t_decomp = perf_counter() - t0\n",
    "    delta_u8 = np.frombuffer(raw, dtype=np.uint8)\n",
    "\n",
    "    # Identify changed elements (any byte in the element changed)\n",
    "    itemsize = np.dtype(meta['dtype_str']).itemsize  # e.g., 4 for float32\n",
    "    # Bytes -> element indices\n",
    "    changed_elem_idx = np.unique(np.flatnonzero(delta_u8) // itemsize)\n",
    "\n",
    "    t1 = perf_counter()\n",
    "    if changed_elem_idx.size == 0:\n",
    "        # Nothing changed\n",
    "        out_features, in_features = meta['shape'][0], meta['shape'][1]\n",
    "        return (np.array([], dtype=np.int64),\n",
    "                np.array([], dtype=np.int64),\n",
    "                np.array([], dtype=np.float32),\n",
    "                (out_features, in_features),\n",
    "                {'t_param_decompress': t_decomp, 't_sparse_extract': 0.0})\n",
    "\n",
    "    # Vectorized element-wise XOR only on the changed rows\n",
    "    base_bytes_all = tensor_to_uint8_view(base_sd[param_name]).reshape(-1, itemsize)\n",
    "    delta_bytes_all = delta_u8.reshape(-1, itemsize)\n",
    "    base_sel = base_bytes_all[changed_elem_idx]\n",
    "    delta_sel = delta_bytes_all[changed_elem_idx]\n",
    "    ft_sel_bytes = np.bitwise_xor(base_sel, delta_sel)\n",
    "    ft_vals = ft_sel_bytes.view(np.dtype(meta['dtype_str'])).reshape(-1)\n",
    "    base_vals = base_sd[param_name].view(-1).detach().cpu().numpy()[changed_elem_idx]\n",
    "    delta_vals = (ft_vals - base_vals).astype(np.float32)\n",
    "\n",
    "    out_features, in_features = meta['shape'][0], meta['shape'][1]\n",
    "    rows = (changed_elem_idx // in_features).astype(np.int64)\n",
    "    cols = (changed_elem_idx % in_features).astype(np.int64)\n",
    "    t_sparse_extract = perf_counter() - t1\n",
    "\n",
    "    return rows, cols, delta_vals, (out_features, in_features), \\\n",
    "           {'t_param_decompress': t_decomp, 't_sparse_extract': t_sparse_extract}\n",
    "\n",
    "\n",
    "def apply_sparse_delta_linear(h2: torch.Tensor, rows, cols, delta_vals, shape, device='cpu'):\n",
    "    \"\"\"Apply y_delta = ΔW @ x for a Linear(out=in_features->out_features) over a 3D input (B, L, in_features).\n",
    "       Returns y_delta with shape (B, L, out_features).\n",
    "    \"\"\"\n",
    "    out_features, in_features = shape\n",
    "    if len(delta_vals) == 0:\n",
    "        return torch.zeros(h2.size(0), h2.size(1), out_features, device=device)\n",
    "\n",
    "    indices = torch.tensor(np.vstack([rows, cols]), dtype=torch.long, device=device)  # (2, nnz)\n",
    "    values = torch.tensor(delta_vals, dtype=torch.float32, device=device)\n",
    "    deltaW = torch.sparse_coo_tensor(indices, values, size=(out_features, in_features), device=device).coalesce()\n",
    "\n",
    "    X = h2.to(device).reshape(-1, in_features)       # (B*L, in)\n",
    "    Y_delta = torch.sparse.mm(deltaW, X.T).T         # (B*L, out)\n",
    "    return Y_delta.reshape(h2.size(0), h2.size(1), out_features)\n",
    "\n",
    "\n",
    "def multitenant_forward_ff0_xor(base_layer: nn.Module, pkg_list: list[dict], x: torch.Tensor, device='cpu'):\n",
    "    \"\"\"Multi-tenant serving prototype:\n",
    "       - Reuse base compute up to h2 and y_base_ff1.\n",
    "       - For each tenant, build sparse ΔW (only for ff.0.weight), compute y_delta = ΔW @ h2,\n",
    "         then finish GELU -> ff.2 -> residual to get final output.\n",
    "       Returns: outputs (list of tensors), timing dict with shared/per-tenant breakdowns.\n",
    "    \"\"\"\n",
    "    base_layer = base_layer.to(device).eval()\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Shared base forward up to h2 and y_base_ff1\n",
    "    t_shared0 = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        h = base_layer.ln1(x)\n",
    "        a, _ = base_layer.self_attn(h, h, h)\n",
    "        x1 = x + a                             # residual before FFN\n",
    "        h2 = base_layer.ln2(x1)\n",
    "        y_base_ff1 = base_layer.ff[0](h2)      # (B, L, dim_ff)\n",
    "    t_shared = perf_counter() - t_shared0\n",
    "\n",
    "    # Pre-bind layers reused for finishing path\n",
    "    gelu = base_layer.ff[1]                    # GELU\n",
    "    ff2  = base_layer.ff[2]                    # second linear\n",
    "\n",
    "    # Build base_sd once (CPU) for delta extraction\n",
    "    base_sd_local = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "\n",
    "    outputs = []\n",
    "    per_tenant = []\n",
    "    for pkg in pkg_list:\n",
    "        t0 = perf_counter()\n",
    "        # Only use ff.0.weight delta\n",
    "        param_name = 'ff.0.weight'\n",
    "        rows, cols, delta_vals, shape, t_part = sparse_delta_from_pkg_for_linear(base_sd_local, pkg, param_name)\n",
    "\n",
    "        # Sparse matmul: y_delta = ΔW @ h2\n",
    "        t1 = perf_counter()\n",
    "        y_delta = apply_sparse_delta_linear(h2, rows, cols, delta_vals, shape, device=device)\n",
    "        t_sparse_mm = perf_counter() - t1\n",
    "\n",
    "        # Merge into ff.0 output, then finish FFN + residual\n",
    "        t2 = perf_counter()\n",
    "        with torch.no_grad():\n",
    "            y_ff1 = y_base_ff1 + y_delta\n",
    "            v = gelu(y_ff1)\n",
    "            w = ff2(v)\n",
    "            y_out = x1 + w\n",
    "        t_finish = perf_counter() - t2\n",
    "\n",
    "        outputs.append(y_out)\n",
    "        per_tenant.append({\n",
    "            't_param_decompress': t_part['t_param_decompress'],\n",
    "            't_sparse_extract': t_part['t_sparse_extract'],\n",
    "            't_sparse_mm': t_sparse_mm,\n",
    "            't_finish': t_finish,\n",
    "            't_total_per_tenant': (t_part['t_param_decompress'] + t_part['t_sparse_extract'] +\n",
    "                                   t_sparse_mm + t_finish),\n",
    "            'nnz_elements': int(len(delta_vals)),\n",
    "        })\n",
    "\n",
    "    timings = {'t_shared_base': t_shared, 'per_tenant': per_tenant}\n",
    "    return outputs, timings\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Demo inputs\n",
    "# -----------------------------\n",
    "B, L, D = 2, 16, 64\n",
    "x_demo = torch.randn(B, L, D)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Baseline E2E per-tenant (full reconstruct) for reference\n",
    "# -----------------------------\n",
    "print(\"\\n=== Baseline E2E (per-tenant, full reconstruct) ===\")\n",
    "for i, pkg in enumerate(pkg_list, 1):\n",
    "    t, _ = e2e_forward_from_pkg(base_layer, pkg, x_demo, device=device)\n",
    "    print(f\"[tenant {i}] decompress={t['t_decompress']:.6f}s  xor={t['t_xor_apply']:.6f}s  \"\n",
    "          f\"state={t['t_state_load']:.6f}s  fwd={t['t_forward']:.6f}s  E2E={t['t_end_to_end']:.6f}s\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Multi-tenant reuse (single shared base path)\n",
    "# -----------------------------\n",
    "print(\"\\n=== Multi-tenant reuse on ff.0.weight (shared base compute) ===\")\n",
    "outs, timing_mt = multitenant_forward_ff0_xor(base_layer, pkg_list, x_demo, device=device)\n",
    "print(f\"shared t_base={timing_mt['t_shared_base']:.6f}s\")\n",
    "for i, t in enumerate(timing_mt['per_tenant'], 1):\n",
    "    print(f\"[tenant {i}] dec={t['t_param_decompress']:.6f}s  sparse_extract={t['t_sparse_extract']:.6f}s  \"\n",
    "          f\"sparse_mm={t['t_sparse_mm']:.6f}s  finish={t['t_finish']:.6f}s  total_tenant={t['t_total_per_tenant']:.6f}s  \"\n",
    "          f\"nnz={t['nnz_elements']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb1feecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Memory impact estimator for multi-tenant XOR\n",
    "# ==========================================\n",
    "\n",
    "def _bytes_str(n):\n",
    "    units = [\"B\",\"KiB\",\"MiB\",\"GiB\",\"TiB\"]\n",
    "    i = 0\n",
    "    x = float(n)\n",
    "    while x >= 1024.0 and i < len(units)-1:\n",
    "        x /= 1024.0\n",
    "        i += 1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "def _floating_param_bytes(sd: dict):\n",
    "    \"\"\"Sum of bytes of all floating-point parameters in a state_dict (CPU tensors).\"\"\"\n",
    "    total = 0\n",
    "    for t in sd.values():\n",
    "        if torch.tensor([], dtype=t.dtype).dtype.is_floating_point:\n",
    "            total += t.numel() * t.element_size()\n",
    "    return total\n",
    "\n",
    "def _count_changed_elements(base_sd: dict, ft_sd: dict):\n",
    "    \"\"\"Count changed float *elements* (not bytes) between base and finetuned.\"\"\"\n",
    "    changed, total = 0, 0\n",
    "    for k in base_sd.keys():\n",
    "        b = base_sd[k]\n",
    "        f = ft_sd[k]\n",
    "        if torch.tensor([], dtype=b.dtype).dtype.is_floating_point:\n",
    "            bb = b.detach().cpu()\n",
    "            ff = f.detach().cpu()\n",
    "            total += bb.numel()\n",
    "            changed += (bb != ff).sum().item()\n",
    "    return int(changed), int(total)\n",
    "\n",
    "def summarize_vram_for_multi_tenant(base_sd, pkg_list, finetuned_modules, tenants=10):\n",
    "    \"\"\"\n",
    "    Estimate VRAM for different strategies if all tenant deltas are kept on GPU memory.\n",
    "    - Strategy A: keep *decompressed XOR bytes* (uint8) for each tenant\n",
    "    - Strategy B: keep *compressed Zstd* bytes for each tenant\n",
    "    - Strategy C: keep *sparse COO* (rows, cols, values) of changed elements (32-bit vs 64-bit indices)\n",
    "\n",
    "    Notes:\n",
    "    - Decompressed XOR bytes size ~= full-precision weight bytes (because XOR is per-byte).\n",
    "    - This does NOT include activation buffers, optimizer states, KV cache, etc. It's weights/deltas only.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base model size (float weights, one copy on GPU)\n",
    "    base_bytes = _floating_param_bytes(base_sd)\n",
    "\n",
    "    # Use per-tenant package stats (assume similar across tenants)\n",
    "    comp_per_tenant = np.mean([p['stats']['total_compressed_bytes'] for p in pkg_list])\n",
    "    rawxor_per_tenant = np.mean([p['stats']['total_delta_bytes'] for p in pkg_list])  # equals float bytes\n",
    "\n",
    "    # Changed elements fraction from provided finetuned modules (ft1, ft2, ...)\n",
    "    ch_fracs = []\n",
    "    for ft in finetuned_modules:\n",
    "        ft_sd_local = {k: v.detach().cpu().contiguous() for k, v in ft.state_dict().items()}\n",
    "        ch, tot = _count_changed_elements(base_sd, ft_sd_local)\n",
    "        ch_fracs.append(ch / max(1, tot))\n",
    "    avg_changed_frac = float(np.mean(ch_fracs)) if ch_fracs else 0.0\n",
    "\n",
    "    # Total float elements in the model (assume homogeneous dtype for estimate)\n",
    "    total_elems = sum(int(t.numel()) for t in base_sd.values()\n",
    "                      if torch.tensor([], dtype=t.dtype).dtype.is_floating_point)\n",
    "\n",
    "    nnz_elems_est = int(round(avg_changed_frac * total_elems))\n",
    "\n",
    "    # Sparse COO memory per element:\n",
    "    # - values: float32 (4B)\n",
    "    # - indices: (row, col). Use 32-bit or 64-bit indices; many frameworks use 32-bit if dims < 2^31.\n",
    "    per_elem_coo32 = 4 + 4 + 4   # 12 bytes\n",
    "    per_elem_coo64 = 4 + 8 + 8   # values fp32 + two int64 indices = 20 bytes\n",
    "\n",
    "    sparse_coo32_per_tenant = nnz_elems_est * per_elem_coo32\n",
    "    sparse_coo64_per_tenant = nnz_elems_est * per_elem_coo64\n",
    "\n",
    "    # Totals on GPU (weights + tenants' deltas in the chosen form)\n",
    "    total_A = base_bytes + tenants * rawxor_per_tenant         # decompressed XOR kept resident\n",
    "    total_B = base_bytes + tenants * comp_per_tenant           # compressed kept resident (for on-the-fly decode/compute)\n",
    "    total_C32 = base_bytes + tenants * sparse_coo32_per_tenant # sparse COO 32-bit idx\n",
    "    total_C64 = base_bytes + tenants * sparse_coo64_per_tenant # sparse COO 64-bit idx\n",
    "\n",
    "    print(\"\\n=== VRAM usage estimate (weights/deltas only) ===\")\n",
    "    print(f\"Base model weights (1 copy, float):   {_bytes_str(base_bytes)}\")\n",
    "    print(f\"Avg per-tenant compressed XOR (Zstd): {_bytes_str(comp_per_tenant)}  \"\n",
    "          f\"(ratio={comp_per_tenant/max(1, rawxor_per_tenant):.4f} vs raw)\")\n",
    "    print(f\"Per-tenant *decompressed* XOR bytes:  {_bytes_str(rawxor_per_tenant)}  (≈ full model bytes)\")\n",
    "\n",
    "    print(f\"\\nChanged elements fraction (avg over provided tenants): {avg_changed_frac*100:.2f}%\")\n",
    "    print(f\"  → Estimated nnz elements per tenant: {nnz_elems_est:,}\")\n",
    "    print(f\"  → Sparse COO per tenant (idx32): {_bytes_str(sparse_coo32_per_tenant)}\")\n",
    "    print(f\"  → Sparse COO per tenant (idx64): {_bytes_str(sparse_coo64_per_tenant)}\")\n",
    "\n",
    "    print(f\"\\n--- If you keep deltas for {tenants} tenants on GPU ---\")\n",
    "    print(f\"A) Keep *decompressed XOR* (uint8): {_bytes_str(total_A)}   \"\n",
    "          f\"[= base {_bytes_str(base_bytes)} + {tenants} × {_bytes_str(rawxor_per_tenant)}]\")\n",
    "    print(f\"B) Keep *compressed* (Zstd level {pkg_list[0]['zstd_level']}): {_bytes_str(total_B)}   \"\n",
    "          f\"[= base {_bytes_str(base_bytes)} + {tenants} × {_bytes_str(comp_per_tenant)}]\")\n",
    "    print(f\"C) Keep *sparse COO* (idx32): {_bytes_str(total_C32)}   \"\n",
    "          f\"[= base {_bytes_str(base_bytes)} + {tenants} × {_bytes_str(sparse_coo32_per_tenant)}]\")\n",
    "    print(f\"   Keep *sparse COO* (idx64): {_bytes_str(total_C64)}\")\n",
    "\n",
    "    print(\"\\nKey takeaway:\")\n",
    "    print(\"- Decompressed XOR per tenant ≈ one full model's worth of bytes. \"\n",
    "          \"With many tenants, VRAM blows up linearly (A).\")\n",
    "    print(\"- Compressed bytes are small (B) but require decode-time compute (decompress→operator fusion or compute in compressed domain).\")\n",
    "    print(\"- Sparse COO (C) scales with actual changed fraction; when changed is rare, it saves VRAM, but requires index and sparse kernel support.\")\n",
    "\n",
    "# ---- Run the summary for N=10 tenants ----\n",
    "base_sd_local = {k: v.detach().cpu().contiguous() for k, v in base_layer.state_dict().items()}\n",
    "summarize_vram_for_multi_tenant(\n",
    "    base_sd=base_sd_local,\n",
    "    pkg_list=pkg_list,\n",
    "    finetuned_modules=[ft1, ft2],   # use the real sparsity of the two tenants to estimate\n",
    "    tenants=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- XOR against a base + Zstd yields **lossless** and often highly compressible deltas when updates are sparse.\n",
    "- We can **exactly** reconstruct the finetuned layer and verify forward-pass equivalence.\n",
    "- The next step (our main research problem) is to **avoid full decompression to avoid memory footprint peak and ensure the computation efficiency**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
